{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Traffic Light Classifier - Model Summery**"
      ],
      "metadata": {
        "id": "KABF0C0c_xtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Let's begin.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Step 1: Define the Problem and Dataset**\n",
        "\n",
        "We’ll simulate a dataset of traffic light color readings based on simplified intensity features:\n",
        "\n",
        "### Inputs (X):\n",
        "\n",
        "* **Red intensity**\n",
        "* **Green intensity**\n",
        "* **Yellow intensity**\n",
        "\n",
        "### Outputs (y):\n",
        "\n",
        "* Classes: `Red`, `Green`, `Yellow`\n",
        "\n",
        "We'll start by encoding those outputs as strings for now.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Icl4h5Iw_3sI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Features: [Red_intensity, Green_intensity, Yellow_intensity]\n",
        "X = np.array([\n",
        "    [1.0, 0.0, 0.0],   # Clearly red\n",
        "    [0.0, 1.0, 0.0],   # Clearly green\n",
        "    [0.0, 0.0, 1.0],   # Clearly yellow\n",
        "    [0.9, 0.1, 0.1],   # Mostly red\n",
        "    [0.1, 0.9, 0.2],   # Mostly green\n",
        "    [0.1, 0.2, 0.9]    # Mostly yellow\n",
        "])\n",
        "\n",
        "# Labels: Target color names\n",
        "y_labels = np.array(['Red', 'Green', 'Yellow', 'Red', 'Green', 'Yellow'])"
      ],
      "metadata": {
        "id": "Tbte7l78AS73"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "✅ Done! We now have a **multi-class classification dataset** with:\n",
        "\n",
        "* 3 input features\n",
        "* 3 output classes"
      ],
      "metadata": {
        "id": "hKoZB9_EAXM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: One-Hot Encode the Output Labels**"
      ],
      "metadata": {
        "id": "368X7TKlC8Si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLPs expect numeric values — especially for multi-class classification.\n",
        "So we need to convert these string labels (\"Red\", \"Green\", \"Yellow\") into one-hot encoded vectors."
      ],
      "metadata": {
        "id": "ZQE-VhNDC8DV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JhJVouwi_EgH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Reshape y_labels to column vector\n",
        "y_labels = y_labels.reshape(-1, 1)\n",
        "\n",
        "# Initialize encoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Transform string labels to one-hot vectors\n",
        "y = encoder.fit_transform(y_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, your y becomes:\n",
        "\n",
        "| Label  | One-hot    |\n",
        "| ------ | ---------- |\n",
        "| Red    | \\[1, 0, 0] |\n",
        "| Green  | \\[0, 1, 0] |\n",
        "| Yellow | \\[0, 0, 1] |\n",
        "\n",
        "One-hot encoding done!"
      ],
      "metadata": {
        "id": "nUTkG39uDQrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Build the MLP architecture.**"
      ],
      "metadata": {
        "id": "Qh47WvCQDV0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Let's move on to the next step.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Step 3: Build the MLP Architecture**\n",
        "\n",
        "We’ll now define the structure of the MLP:\n",
        "\n",
        "* **Input layer**: 3 neurons (Red, Green, Yellow intensity)\n",
        "* **Hidden layer**: Let’s use 4 neurons (you can tune this)\n",
        "* **Output layer**: 3 neurons (Red, Green, Yellow) → softmax output\n",
        "\n",
        "### Activation choices:\n",
        "\n",
        "* Hidden: **ReLU**\n",
        "* Output: **Softmax** (for multi-class classification)\n",
        "\n",
        "We’ll use **NumPy only**, so we’ll write everything ourselves — no frameworks yet.\n",
        "\n"
      ],
      "metadata": {
        "id": "4N70jxRtDVdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer sizes\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 3\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n"
      ],
      "metadata": {
        "id": "MXp6CglFDWeJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ This initializes:\n",
        "\n",
        "* `W1` and `b1`: from input → hidden\n",
        "* `W2` and `b2`: from hidden → output\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "n7GJnVEXEBZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Why Did We Use 4 Neurons in the Hidden Layer?\n",
        "\n",
        "### 1. ✅ **Heuristic Choice (Rule of Thumb)**\n",
        "\n",
        "In small models, we often start with:\n",
        "\n",
        "> A number **between input size and output size × 2**\n",
        "\n",
        "* Input size = 3\n",
        "* Output size = 3\n",
        "  So:\n",
        "* Hidden size between 3 and 6 is usually a good starting point\n",
        "* We picked 4 as a balanced middle\n",
        "\n",
        "---\n",
        "\n",
        "### 2. ✅ **Simplicity First**\n",
        "\n",
        "* We **don’t want to overcomplicate** with too many neurons.\n",
        "* More neurons = more weights = more training time + higher risk of overfitting.\n",
        "* Since the dataset is small and simple (6 examples), 4 neurons keep the model lightweight.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. 📊 **Can We Tune It Later?**\n",
        "\n",
        "Yes. we can **experiment** with:\n",
        "\n",
        "* 2 neurons (underfitting likely)\n",
        "* 8 or 16 neurons (might overfit)\n",
        "\n",
        "Use this as a **baseline**, and adjust based on:\n",
        "\n",
        "* Validation accuracy\n",
        "* Overfitting/underfitting signs\n",
        "* Loss curve behavior\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Summary:\n",
        "\n",
        "> We chose 4 hidden neurons because it’s a **balanced, safe starting point** for a small 3-input, 3-output task.\n",
        "> You can **adjust it later** based on how the model performs.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "q2usQp56EH0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Implement forward pass with ReLU and Softmax**"
      ],
      "metadata": {
        "id": "Wb6FarM-EITi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What happens in a forward pass?\n",
        "\n",
        "1. Inputs go through **linear transformation** to the hidden layer\n",
        "2. **ReLU** activation is applied to introduce non-linearity\n",
        "3. Hidden layer output is passed to the output layer\n",
        "4. **Softmax** is applied to get class probabilities\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "shrtIE2PFLD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ReLU activation (for hidden layer)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Softmax activation (for output layer)\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))  # numerical stability\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "# Forward pass function\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, W1) + b1        # Input → Hidden\n",
        "    a1 = relu(z1)                  # Apply ReLU\n",
        "    z2 = np.dot(a1, W2) + b2       # Hidden → Output\n",
        "    a2 = softmax(z2)               # Apply Softmax\n",
        "    return z1, a1, z2, a2\n"
      ],
      "metadata": {
        "id": "PFum6OCdFpBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This function returns:\n",
        "\n",
        "* `z1`: Linear output before ReLU\n",
        "* `a1`: Activated hidden layer\n",
        "* `z2`: Output before softmax\n",
        "* `a2`: Final output probabilities (shape: \\[n\\_samples, 3])\n",
        "\n",
        "---\n",
        "\n",
        "✅ Forward pass is ready!"
      ],
      "metadata": {
        "id": "r-6sE8iJFtFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Loss calculation and backpropagation**"
      ],
      "metadata": {
        "id": "oqsfMwwRF7FR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📦 Categorical Cross-Entropy Loss (for multi-class)\n",
        "\n",
        "Used when:\n",
        "\n",
        "* Output = one-hot vector\n",
        "* Final layer = softmax\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f_UXYsa1F7AB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_cross_entropy(y_true, y_pred):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))"
      ],
      "metadata": {
        "id": "wDhrttooGqow"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* `y_true`: one-hot encoded true labels (e.g., `[1, 0, 0]`)\n",
        "* `y_pred`: softmax probabilities\n",
        "* The small `1e-8` avoids log(0)\n"
      ],
      "metadata": {
        "id": "xwn8OkmuGq9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 🔁 Backpropagation Steps\n",
        "\n",
        "We'll derive gradients layer-by-layer:\n",
        "\n",
        "| Layer        | Formula                                            |\n",
        "| ------------ | -------------------------------------------------- |\n",
        "| Output error | $\\text{dZ2} = \\text{a2} - y$                       |\n",
        "| Output grads | $\\text{dW2} = a1^T \\cdot dZ2$, $db2 = \\sum dZ2$    |\n",
        "| Hidden error | $\\text{dZ1} = (dZ2 \\cdot W2^T) * \\text{ReLU}'(z1)$ |\n",
        "| Hidden grads | $\\text{dW1} = X^T \\cdot dZ1$, $db1 = \\sum dZ1$     |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8UftquEaG1G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU derivative (used in hidden layer backprop)\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "# Backpropagation\n",
        "def backward(X, y, z1, a1, z2, a2, learning_rate):\n",
        "    # Update global weights and biases\n",
        "    global W1, b1, W2, b2\n",
        "\n",
        "    m = X.shape[0]  # number of samples\n",
        "\n",
        "    # Output layer error\n",
        "    dZ2 = a2 - y\n",
        "    dW2 = np.dot(a1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Hidden layer error\n",
        "    dZ1 = np.dot(dZ2, W2.T) * relu_derivative(z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2"
      ],
      "metadata": {
        "id": "24GRS2Q7HCTd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "✅ Now you’ve implemented:\n",
        "\n",
        "* Softmax + Cross-Entropy loss\n",
        "* Layer-by-layer gradient flow\n",
        "* Manual parameter updates"
      ],
      "metadata": {
        "id": "QOBQ2dTKHHp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Training loop**"
      ],
      "metadata": {
        "id": "wRoeEPdpHvMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will :\n",
        "\n",
        "* Call forward() on input data\n",
        "\n",
        "* Calculate the loss\n",
        "\n",
        "* Run backward() to update weights\n",
        "\n",
        "* Repeat over multiple epochs"
      ],
      "metadata": {
        "id": "rxJi2Dk9HwO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Training loop\n",
        "def train(X, y, epochs=1000, learning_rate=0.1):\n",
        "    for epoch in range(epochs):\n",
        "        z1, a1, z2, a2 = forward(X)                  # Forward pass\n",
        "        loss = categorical_cross_entropy(y, a2)      # Compute loss\n",
        "        backward(X, y, z1, a1, z2, a2, learning_rate) # Backpropagation\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "EPQfDgxjH7-Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the Model**"
      ],
      "metadata": {
        "id": "i4BhaPXKH-Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(X, y, epochs=1000, learning_rate=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRKM-dDLICG6",
        "outputId": "5c5ef0c4-5ede-43c0-fae1-09b28023ccf1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.6488\n",
            "Epoch 100, Loss: 0.1453\n",
            "Epoch 200, Loss: 0.0366\n",
            "Epoch 300, Loss: 0.0176\n",
            "Epoch 400, Loss: 0.0109\n",
            "Epoch 500, Loss: 0.0077\n",
            "Epoch 600, Loss: 0.0059\n",
            "Epoch 700, Loss: 0.0047\n",
            "Epoch 800, Loss: 0.0039\n",
            "Epoch 900, Loss: 0.0033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Predict and Evaluate Output.**"
      ],
      "metadata": {
        "id": "uXYy387ZIIbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll:\n",
        "\n",
        "Run a forward pass using trained weights\n",
        "\n",
        "Get final predicted probabilities\n",
        "\n",
        "Convert probabilities to class labels using argmax\n",
        "\n",
        "Compare with true labels"
      ],
      "metadata": {
        "id": "Df3oY0GLK4qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final predictions\n",
        "_, _, _, output_probs = forward(X)\n",
        "\n",
        "# Convert softmax probabilities to predicted class index (0, 1, 2)\n",
        "predicted_indices = np.argmax(output_probs, axis=1)\n",
        "\n",
        "# Convert one-hot true labels back to indices\n",
        "true_indices = np.argmax(y, axis=1)\n",
        "\n",
        "# Reverse map index → label name using encoder\n",
        "predicted_labels = encoder.inverse_transform(output_probs)\n",
        "true_labels = encoder.inverse_transform(y)\n",
        "\n",
        "# Display\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "# Create and print results table\n",
        "results_df = pd.DataFrame({\n",
        "    \"Red Intensity\": X[:, 0],\n",
        "    \"Green Intensity\": X[:, 1],\n",
        "    \"Yellow Intensity\": X[:, 2],\n",
        "    \"Predicted\": predicted_labels.flatten(),\n",
        "    \"Actual\": true_labels.flatten()\n",
        "})\n",
        "\n",
        "print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxZOoKmdIG_A",
        "outputId": "02cd9233-45ac-448a-a5f0-6f6ff90b6d7e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Red Intensity  Green Intensity  Yellow Intensity Predicted  Actual\n",
            "0            1.0              0.0               0.0       Red     Red\n",
            "1            0.0              1.0               0.0     Green   Green\n",
            "2            0.0              0.0               1.0    Yellow  Yellow\n",
            "3            0.9              0.1               0.1       Red     Red\n",
            "4            0.1              0.9               0.2     Green   Green\n",
            "5            0.1              0.2               0.9    Yellow  Yellow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a fully working MLP that:\n",
        "\n",
        "* Trains on simple intensity features\n",
        "\n",
        "* Learns to classify Red, Yellow, Green\n",
        "\n",
        "* Uses forward + backprop + softmax + cross-entropy"
      ],
      "metadata": {
        "id": "UKNWUVMdLlGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New test inputs (unseen by training)\n",
        "X_test = np.array([\n",
        "    [0.95, 0.05, 0.05],  # Very red\n",
        "    [0.2, 0.7, 0.2],     # Very green\n",
        "    [0.1, 0.2, 0.75],    # Very yellow\n",
        "    [0.4, 0.3, 0.3]      # Balanced mix\n",
        "])\n",
        "\n",
        "# Run forward pass to get predictions\n",
        "_, _, _, test_output_probs = forward(X_test)\n",
        "\n",
        "# Convert softmax outputs to labels\n",
        "test_predicted_labels = encoder.inverse_transform(test_output_probs)\n",
        "\n",
        "# Show results\n",
        "import pandas as pd\n",
        "\n",
        "test_df = pd.DataFrame({\n",
        "    \"Red Intensity\": X_test[:, 0],\n",
        "    \"Green Intensity\": X_test[:, 1],\n",
        "    \"Yellow Intensity\": X_test[:, 2],\n",
        "    \"Predicted Label\": test_predicted_labels.flatten()\n",
        "})\n",
        "\n",
        "print(test_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiO0Mc5iNSyA",
        "outputId": "134bef8f-052d-4c11-a19e-87a35e639154"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Red Intensity  Green Intensity  Yellow Intensity Predicted Label\n",
            "0           0.95             0.05              0.05             Red\n",
            "1           0.20             0.70              0.20           Green\n",
            "2           0.10             0.20              0.75          Yellow\n",
            "3           0.40             0.30              0.30             Red\n"
          ]
        }
      ]
    }
  ]
}