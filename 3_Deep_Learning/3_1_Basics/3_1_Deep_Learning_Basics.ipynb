{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Perceptron**"
      ],
      "metadata": {
        "id": "BYK727p3I4m3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìå 1. Technical Introduction\n",
        "\n",
        "### üß≠ Where It Fits:\n",
        "\n",
        "* The **Perceptron** is the simplest form of a **neural network** ‚Äî it‚Äôs a **single-layer, single-neuron model**.\n",
        "* It belongs to **supervised learning**.\n",
        "* It was originally designed for **binary classification** problems (e.g., yes/no, 0/1, cat/not-cat).\n",
        "\n",
        "### üõ† How It Works Conceptually:\n",
        "\n",
        "* Takes multiple inputs, applies weights to them, adds a bias, and passes the result through an **activation function**.\n",
        "* If the result is above a threshold, it outputs 1; otherwise, it outputs 0.\n",
        "\n",
        "---\n",
        "\n",
        "## üß∏ 2. Simplified Explanation\n",
        "\n",
        "Imagine a **voting machine**:\n",
        "\n",
        "* Each input (like ‚Äúis the road clear?‚Äù, ‚Äúis speed safe?‚Äù) **votes** with a certain **weight**.\n",
        "* The machine **adds up the votes**, and if the total is above a limit, it says ‚ÄúGo‚Äù (1); otherwise, ‚ÄúStop‚Äù (0).\n",
        "\n",
        "That‚Äôs what a Perceptron does ‚Äî **a smart yes/no decider**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìï 3. Definition\n",
        "\n",
        "> A **Perceptron** is a linear binary classifier that computes a weighted sum of input features, adds a bias, and passes the result through an activation function (usually a step function) to produce a binary output (0 or 1).\n",
        "\n",
        "---\n",
        "Got it! For foundational concepts like perceptrons or activations, I‚Äôll keep the explanations natural and intuitive ‚Äî no rigid #Explain format. Let‚Äôs continue. Here's the rest of the explanation on **Perceptron**:\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Perceptron: How It Works (Simple Math)\n",
        "\n",
        "Given:\n",
        "\n",
        "* Inputs: $x_1, x_2, ..., x_n$\n",
        "* Weights: $w_1, w_2, ..., w_n$\n",
        "* Bias: $b$\n",
        "\n",
        "The perceptron calculates:\n",
        "\n",
        "$$\n",
        "z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b\n",
        "$$\n",
        "\n",
        "Then it applies an **activation function**. In a basic perceptron, it‚Äôs a **step function**:\n",
        "\n",
        "$$\n",
        "\\text{output} =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } z \\geq 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Example:\n",
        "\n",
        "Say a car should go if:\n",
        "\n",
        "* Road is clear ($x_1 = 1$)\n",
        "* Speed is safe ($x_2 = 1$)\n",
        "\n",
        "You assign:\n",
        "\n",
        "* $w_1 = 0.6, w_2 = 0.4, b = -0.8$\n",
        "\n",
        "$$\n",
        "z = (0.6 \\cdot 1) + (0.4 \\cdot 1) - 0.8 = 0.2 \\Rightarrow \\text{output} = 1 \\ (\\text{Go})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What Perceptron Can and Cannot Do\n",
        "\n",
        "### What It Can Do:\n",
        "\n",
        "* Learn simple binary classification (e.g., yes/no)\n",
        "* Solve **linearly separable** problems (like AND, OR logic)\n",
        "\n",
        "### What It Can‚Äôt Do:\n",
        "\n",
        "* **Cannot solve XOR** or complex problems\n",
        "* Can‚Äôt learn curved or abstract patterns ‚Äî for that, we need **multi-layer networks**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WsOCL78oI-IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Model Example - Learn the OR function**"
      ],
      "metadata": {
        "id": "CYPqaZ0CPuvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üî¢ OR Truth Table:\n",
        "\n",
        "| x1 | x2 | Output |\n",
        "| -- | -- | ------ |\n",
        "| 0  | 0  | 0      |\n",
        "| 0  | 1  | 1      |\n",
        "| 1  | 0  | 1      |\n",
        "| 1  | 1  | 1      |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Perceptron Formula\n",
        "\n",
        "$$\n",
        "\\text{output} =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } (w \\cdot x + b) \\geq 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "We‚Äôll use:\n",
        "\n",
        "* **Step function** as activation\n",
        "* **Manual training with epochs**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DBEm4HlnJ9fB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkoFIesFJxaN",
        "outputId": "78d98b32-09b4-4949-fd21-984421aa5605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Input: [0 0], Target: 0, Prediction: 1, Error: -1\n",
            "Input: [0 1], Target: 1, Prediction: 0, Error: 1\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.  0.1], Bias: 0.0\n",
            "\n",
            "Epoch 2\n",
            "Input: [0 0], Target: 0, Prediction: 1, Error: -1\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 0, Error: 1\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: 0.0\n",
            "\n",
            "Epoch 3\n",
            "Input: [0 0], Target: 0, Prediction: 1, Error: -1\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "\n",
            "Epoch 4\n",
            "Input: [0 0], Target: 0, Prediction: 0, Error: 0\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "\n",
            "Epoch 5\n",
            "Input: [0 0], Target: 0, Prediction: 0, Error: 0\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "\n",
            "Epoch 6\n",
            "Input: [0 0], Target: 0, Prediction: 0, Error: 0\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "\n",
            "Epoch 7\n",
            "Input: [0 0], Target: 0, Prediction: 0, Error: 0\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "\n",
            "Epoch 8\n",
            "Input: [0 0], Target: 0, Prediction: 0, Error: 0\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "\n",
            "Epoch 9\n",
            "Input: [0 0], Target: 0, Prediction: 0, Error: 0\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "\n",
            "Epoch 10\n",
            "Input: [0 0], Target: 0, Prediction: 0, Error: 0\n",
            "Input: [0 1], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 0], Target: 1, Prediction: 1, Error: 0\n",
            "Input: [1 1], Target: 1, Prediction: 1, Error: 0\n",
            "Weights: [0.1 0.1], Bias: -0.1\n",
            "\n",
            "Final predictions:\n",
            "[0 0] => 0\n",
            "[0 1] => 1\n",
            "[1 0] => 1\n",
            "[1 1] => 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step activation function\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "# Perceptron class\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.1):\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def predict(self, x):\n",
        "        z = np.dot(self.weights, x) + self.bias\n",
        "        return step_function(z)\n",
        "\n",
        "    def train(self, X, y, epochs=10):\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch+1}\")\n",
        "            for xi, target in zip(X, y):\n",
        "                prediction = self.predict(xi)\n",
        "                error = target - prediction\n",
        "                # Weight and bias update rule\n",
        "                self.weights += self.lr * error * xi\n",
        "                self.bias += self.lr * error\n",
        "                print(f\"Input: {xi}, Target: {target}, Prediction: {prediction}, Error: {error}\")\n",
        "            print(f\"Weights: {self.weights}, Bias: {self.bias}\\n\")\n",
        "\n",
        "# Training data for OR\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 1, 1, 1])\n",
        "\n",
        "# Create and train perceptron\n",
        "p = Perceptron(input_size=2)\n",
        "p.train(X, y)\n",
        "\n",
        "# Test\n",
        "print(\"Final predictions:\")\n",
        "for xi in X:\n",
        "    print(f\"{xi} => {p.predict(xi)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ùå Why XOR Does Not Work with a Perceptron\n"
      ],
      "metadata": {
        "id": "gTkDGWLCS4ea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's look at the XOR truth table:\n",
        "\n",
        "| x1 | x2 | XOR Output |\n",
        "| -- | -- | ---------- |\n",
        "| 0  | 0  | 0          |\n",
        "| 0  | 1  | 1          |\n",
        "| 1  | 0  | 1          |\n",
        "| 1  | 1  | 0          |\n",
        "\n",
        "Now **plot these points**, and you‚Äôll see:\n",
        "\n",
        "* You cannot **draw a straight line** that separates the 1s from the 0s.\n",
        "* That means the data is **not linearly separable**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What Does a Perceptron Do?\n",
        "\n",
        "A **single-layer perceptron** can only classify data that is **linearly separable** ‚Äî meaning:\n",
        "\n",
        "> It can only draw **a straight decision boundary** between two classes.\n",
        "\n",
        "Since XOR needs a **non-linear boundary**, the simple perceptron fails.\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† Example: What Happens If You Try XOR?\n",
        "\n",
        "If you use the same perceptron code as before and train it on XOR:\n",
        "\n",
        "```python\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "```\n",
        "\n",
        "üîÅ No matter how many epochs you run:\n",
        "\n",
        "* The perceptron **won‚Äôt learn XOR correctly**.\n",
        "* It will just keep adjusting weights without reaching zero error.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ How to Solve XOR?\n",
        "\n",
        "You need to use a **Multi-Layer Perceptron (MLP)** ‚Äî also called a **Neural Network** with:\n",
        "\n",
        "* **Input layer**\n",
        "* **Hidden layer** (this is key!)\n",
        "* **Output layer**\n",
        "\n",
        "The **hidden layer adds non-linearity**, which allows the model to learn complex patterns like XOR.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "| Model                   | Can Learn XOR? | Why Not / Why Yes?                      |\n",
        "| ----------------------- | -------------- | --------------------------------------- |\n",
        "| Single-Layer Perceptron | ‚ùå No           | Can only model linear boundaries        |\n",
        "| Multi-Layer Perceptron  | ‚úÖ Yes          | Hidden layers add the needed complexity |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TNlOizbjSz7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multi-Layer Perceptron**"
      ],
      "metadata": {
        "id": "bCzD3pBSUW_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† What Is a Multi-Layer Perceptron?\n",
        "\n",
        "A **Multi-Layer Perceptron (MLP)** is a **neural network with at least one hidden layer** between input and output.\n",
        "\n",
        "It can learn **non-linear patterns** like XOR, which a single-layer perceptron cannot.\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è Basic Structure of an MLP\n",
        "\n",
        "```\n",
        "Input Layer ‚Üí Hidden Layer(s) ‚Üí Output Layer\n",
        "```\n",
        "\n",
        "Each layer consists of **neurons**, and each neuron:\n",
        "\n",
        "1. Takes weighted input\n",
        "2. Adds bias\n",
        "3. Applies activation function (like ReLU, Sigmoid)\n",
        "\n",
        "---\n",
        "\n",
        "### üßæ Example MLP Architecture (XOR problem)\n",
        "\n",
        "```\n",
        "Input:  x1, x2\n",
        "       ‚Üì   ‚Üì\n",
        "    [Hidden Layer]\n",
        "       ‚Üì   ‚Üì\n",
        "   Output: y\n",
        "```\n",
        "\n",
        "Even a **2-2-1** network can solve XOR:\n",
        "\n",
        "* 2 input neurons\n",
        "* 1 hidden layer with 2 neurons\n",
        "* 1 output neuron\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è How MLP Learns?\n",
        "\n",
        "### Step-by-step:\n",
        "\n",
        "1. **Forward Propagation**: Pass input through all layers to get prediction\n",
        "2. **Loss Calculation**: Compare prediction with actual output\n",
        "3. **Backpropagation**: Calculate gradients and update weights\n",
        "4. Repeat over many **epochs**\n",
        "\n",
        "This is called **training** the network.\n",
        "\n",
        "---\n",
        "\n",
        "## üìê Activation Functions in MLP\n",
        "\n",
        "Hidden layers must use **non-linear activation functions** like:\n",
        "\n",
        "* **ReLU** (faster, preferred in modern networks)\n",
        "* **Sigmoid** (used in binary output)\n",
        "* **Tanh**\n",
        "\n",
        "These allow the network to **approximate complex curves**, not just lines.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What Makes MLP Powerful?\n",
        "\n",
        "* Can learn **non-linear decision boundaries**\n",
        "* Can model **complex functions**\n",
        "* Is the basis of all **deep learning architectures**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jY0tmRelURTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# === Step 1: Define the dataset ===\n",
        "# Input: 2 features; Output: XOR truth table\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "# === Step 2: Define activation function and its derivative ===\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))  # squashes values between 0 and 1\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)  # derivative of sigmoid used for backpropagation\n",
        "\n",
        "# === Step 3: Initialize weights and biases ===\n",
        "np.random.seed(0)  # ensure reproducibility\n",
        "\n",
        "input_size = 2     # two input features (x1, x2)\n",
        "hidden_size = 2    # two neurons in hidden layer (enough to solve XOR)\n",
        "output_size = 1    # one output neuron (binary output)\n",
        "\n",
        "# Randomly initialize weights for both layers\n",
        "W1 = np.random.randn(input_size, hidden_size)  # shape: (2,2)\n",
        "b1 = np.zeros((1, hidden_size))                # bias for hidden layer\n",
        "\n",
        "W2 = np.random.randn(hidden_size, output_size) # shape: (2,1)\n",
        "b2 = np.zeros((1, output_size))                # bias for output layer\n",
        "\n",
        "# === Step 4: Define loss function ===\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    # Added small epsilon to prevent log(0)\n",
        "    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
        "\n",
        "# === Step 5: Training loop ===\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # === Forward Pass ===\n",
        "    z1 = np.dot(X, W1) + b1        # Linear activation for hidden layer\n",
        "    a1 = sigmoid(z1)               # Non-linear activation for hidden layer\n",
        "\n",
        "    z2 = np.dot(a1, W2) + b2       # Linear activation for output layer\n",
        "    a2 = sigmoid(z2)               # Final prediction after sigmoid\n",
        "\n",
        "    # === Compute Loss ===\n",
        "    loss = binary_cross_entropy(y, a2)\n",
        "\n",
        "    # === Backpropagation ===\n",
        "    # Gradient of loss with respect to output activation\n",
        "    error_output = a2 - y\n",
        "\n",
        "    # Gradients for W2 and b2\n",
        "    dW2 = np.dot(a1.T, error_output)\n",
        "    db2 = np.sum(error_output, axis=0, keepdims=True)\n",
        "\n",
        "    # Backprop to hidden layer\n",
        "    error_hidden = np.dot(error_output, W2.T) * sigmoid_derivative(z1)\n",
        "    dW1 = np.dot(X.T, error_hidden)\n",
        "    db1 = np.sum(error_hidden, axis=0, keepdims=True)\n",
        "\n",
        "    # === Update weights and biases ===\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    # Print loss occasionally\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# === Step 6: Final Predictions ===\n",
        "print(\"\\nFinal predictions after training:\")\n",
        "for i in range(4):\n",
        "    z1 = np.dot(X[i], W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "    print(f\"Input: {X[i]}, Predicted: {a2[0][0]:.4f}, Actual: {y[i][0]}\")\n"
      ],
      "metadata": {
        "id": "GDIX4adU7y84",
        "outputId": "c8883a72-b849-4107-fcc1-64bc02cfd792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.7339\n",
            "Epoch 1000, Loss: 0.3714\n",
            "Epoch 2000, Loss: 0.3563\n",
            "Epoch 3000, Loss: 0.3525\n",
            "Epoch 4000, Loss: 0.3507\n",
            "Epoch 5000, Loss: 0.3498\n",
            "Epoch 6000, Loss: 0.3492\n",
            "Epoch 7000, Loss: 0.3488\n",
            "Epoch 8000, Loss: 0.3485\n",
            "Epoch 9000, Loss: 0.3482\n",
            "\n",
            "Final predictions after training:\n",
            "Input: [0 0], Predicted: 0.0014, Actual: 0\n",
            "Input: [0 1], Predicted: 0.4995, Actual: 1\n",
            "Input: [1 0], Predicted: 0.9982, Actual: 1\n",
            "Input: [1 1], Predicted: 0.5009, Actual: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zbv0l2dLHys5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3qn3NI4i71bL"
      }
    }
  ]
}