{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision_Tree_Regression**"
      ],
      "metadata": {
        "id": "jCZZnv3xcqzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‚úÖ What Is It?**\n",
        "\n",
        "A Decision Tree Regressor splits your dataset into segments by asking yes/no questions on feature values ‚Äî like a flowchart ‚Äî to predict a continuous target.\n",
        "\n",
        "Think of it like a \"20 Questions\" game that splits the data until it gets close enough to a target value."
      ],
      "metadata": {
        "id": "_JnrTdfCcuQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Let‚Äôs begin your next regression model: **Decision Tree Regression**, and as per your preference, I‚Äôll explain everything comprehensively ‚Äî not just how, but why and when to use it.\n",
        "\n",
        "---\n",
        "\n",
        "## üå≥ Decision Tree Regression ‚Äì Step 1: The Basics\n",
        "\n",
        "### ‚úÖ What Is It?\n",
        "\n",
        "A **Decision Tree Regressor** splits your dataset into segments by asking **yes/no questions** on feature values ‚Äî like a flowchart ‚Äî to predict a continuous target.\n",
        "\n",
        "> Think of it like a \"20 Questions\" game that splits the data until it gets close enough to a target value.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Example\n",
        "\n",
        "```plaintext\n",
        "Is Present_Price > 7?\n",
        "‚îú‚îÄ‚îÄ Yes: Is Car_Age > 3?\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Yes ‚Üí predict 4.5\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ No  ‚Üí predict 6.2\n",
        "‚îî‚îÄ‚îÄ No: predict 2.8\n",
        "```\n",
        "\n",
        "Each leaf node gives a predicted `Selling_Price`.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Step 2: Why It's Different (Compared to Linear/Lasso/Ridge/ElasticNet/Polynomial)\n",
        "\n",
        "| Feature                  | Decision Tree               | Linear Models (Ridge, Lasso, etc.) |\n",
        "| ------------------------ | --------------------------- | ---------------------------------- |\n",
        "| Model type               | Non-parametric              | Parametric (learn weights)         |\n",
        "| Handles non-linearity    | ‚úÖ Yes                       | ‚ùå (only Polynomial could handle)   |\n",
        "| Handles interactions     | ‚úÖ Yes (automatically)       | ‚ùå Only via PolynomialFeatures      |\n",
        "| Feature scaling needed?  | ‚ùå No                        | ‚úÖ Yes (e.g. StandardScaler)        |\n",
        "| Handles categorical data | ‚úÖ Yes (with label encoding) | ‚ùå No, needs one-hot                |\n",
        "| Interpretability         | ‚úÖ Yes (tree)                | ‚úÖ (coefficients)                   |\n",
        "| Overfitting risk         | ‚ö†Ô∏è High if not pruned       | Moderate                           |\n",
        "| Feature selection        | ‚úÖ Yes (via splits)          | ‚ùå Only Lasso & ElasticNet          |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úçÔ∏è Step 3: Math Behind It (Simple & Practical)\n",
        "\n",
        "### Goal:\n",
        "\n",
        "At each split in the tree, we want to **minimize the variance (MSE)** of the target in each branch.\n",
        "\n",
        "### Split Logic:\n",
        "\n",
        "At every feature and value split:\n",
        "\n",
        "```python\n",
        "MSE_split = (n_left / n_total) * MSE_left + (n_right / n_total) * MSE_right\n",
        "```\n",
        "\n",
        "It selects the feature and value that gives the **lowest total MSE** after the split.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QkNFuL6deeaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≥ **What is Decision Tree Regression?**\n",
        "\n",
        "It‚Äôs a model that makes predictions by **splitting the data step-by-step**, asking simple yes/no questions at each stage.\n",
        "\n",
        "Think of it as a **\"game of 20 questions\"** ‚Äî but for numbers.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Goal: Predict a number (like `Selling_Price`)\n",
        "\n",
        "Instead of trying to draw a straight line (like linear regression), it:\n",
        "\n",
        "1. Looks at the data.\n",
        "2. Finds a feature and value that splits it into 2 parts that are **more ‚Äúpure‚Äù** (less varied).\n",
        "3. Keeps splitting those parts again and again, like a flowchart.\n",
        "4. At the end (leaf node), it **predicts the average of the samples in that group**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Example (Very Simplified)\n",
        "\n",
        "Imagine this tiny dataset:\n",
        "\n",
        "| Car\\_Age | Selling\\_Price |\n",
        "| -------- | -------------- |\n",
        "| 1        | 8.0            |\n",
        "| 2        | 7.5            |\n",
        "| 3        | 6.5            |\n",
        "| 7        | 3.0            |\n",
        "| 8        | 2.5            |\n",
        "\n",
        "Let‚Äôs say we want to split this into 2 groups.\n",
        "\n",
        "* The algorithm checks: \"What if I split at Car\\_Age = 4?\"\n",
        "\n",
        "  * Group A: Age ‚â§ 4 ‚Üí prices = \\[8.0, 7.5, 6.5]\n",
        "  * Group B: Age > 4 ‚Üí prices = \\[3.0, 2.5]\n",
        "\n",
        "Now it asks:\n",
        "\n",
        "> ‚ÄúIs this split better than splitting at Car\\_Age = 6?‚Äù\n",
        "\n",
        "It repeats this search on all features.\n",
        "\n",
        "---\n",
        "\n",
        "## üìè How Does It Measure ‚ÄúBetter‚Äù?\n",
        "\n",
        "It uses something called **Mean Squared Error (MSE)**:\n",
        "\n",
        "* Lower MSE = better prediction\n",
        "* It chooses the split that gives the **lowest combined MSE** for both branches\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Then What Happens?\n",
        "\n",
        "* Once the best first split is found, it **recursively splits again** in both branches.\n",
        "* This continues until:\n",
        "\n",
        "  * A max depth is reached\n",
        "  * The MSE is very low (pure group)\n",
        "  * There are too few samples left\n",
        "\n",
        "---\n",
        "\n",
        "### üå≥ Visual Structure of Tree\n",
        "\n",
        "```plaintext\n",
        "Q1: Is Present_Price > 6?\n",
        "‚îú‚îÄ‚îÄ Yes ‚Üí Q2: Is Car_Age > 3?\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ Yes ‚Üí predict 4.2\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ No  ‚Üí predict 6.8\n",
        "‚îî‚îÄ‚îÄ No  ‚Üí predict 3.1\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Concept Summary\n",
        "\n",
        "* It‚Äôs not trying to fit a line, it‚Äôs **cutting data into rectangles**\n",
        "* It works well when:\n",
        "\n",
        "  * Data is non-linear\n",
        "  * Data has rule-like patterns (common in real-world systems like cars)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4OgtGoCme3YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üéØ Problem: We have many features like:\n",
        "\n",
        "```\n",
        "['Present_Price', 'Car_Age', 'Kms_Driven', 'Owner', ...]\n",
        "```\n",
        "\n",
        "So how does the tree decide **where to split and which feature to use**?\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Step-by-Step: How Splitting Works with Multiple Features\n",
        "\n",
        "### üîÅ For Every Split Level in the Tree:\n",
        "\n",
        "1. **Look at all features**, one by one.\n",
        "2. For each feature, try **many possible split points**:\n",
        "\n",
        "   * e.g. For `Present_Price`: try splitting at 4.5, 6.0, 8.0 etc.\n",
        "3. For each candidate split:\n",
        "\n",
        "   * Divide the data into left/right groups\n",
        "   * Compute **Mean Squared Error (MSE)** for that split\n",
        "4. Choose the split (feature + value) that gives the **lowest MSE**.\n",
        "\n",
        "> ‚úÖ This is called **greedy splitting**, because it always chooses the **best split at that level only**, not globally.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Mini Example\n",
        "\n",
        "Suppose we have:\n",
        "\n",
        "```python\n",
        "features = ['Present_Price', 'Car_Age']\n",
        "```\n",
        "\n",
        "At the root level, it tries:\n",
        "\n",
        "* `Present_Price <= 5` ‚Üí MSE = 4.1\n",
        "* `Car_Age <= 3` ‚Üí MSE = 2.9 ‚úÖ\n",
        "\n",
        "It picks `Car_Age <= 3`, because it gives the **lowest MSE**.\n",
        "\n",
        "Then on the left and right subsets, it repeats the same process:\n",
        "\n",
        "* Try both features again\n",
        "* Find best local split\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ What If You Have 100+ Features?\n",
        "\n",
        "It still works the same:\n",
        "\n",
        "* It checks **each feature independently**\n",
        "* Picks the **best feature + split point** at every level\n",
        "* Automatically **ignores unhelpful features** (they‚Äôre not chosen for any splits)\n",
        "\n",
        "> ‚ö†Ô∏è No regularization is built-in ‚Üí tree can grow too complex without pruning\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "| Concept           | Decision Tree Behavior                 |\n",
        "| ----------------- | -------------------------------------- |\n",
        "| Multiple features | Tried one-by-one at each split         |\n",
        "| Split chosen by   | Lowest MSE from all candidates         |\n",
        "| Feature selection | Done **implicitly** via splitting      |\n",
        "| Multicollinearity | Doesn‚Äôt matter ‚Äî not coefficient-based |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YsUMYo8ejAjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **‚úÇÔ∏è What is Pruning in Decision Trees?**\n",
        "\n",
        "**Pruning** means:\n",
        "\n",
        "> \"Stop the tree from growing too deep or cut it back after it's grown.\"\n",
        "\n",
        "It‚Äôs used to prevent **overfitting**, where the model becomes too complex and memorizes the training data.\n",
        "\n",
        "---\n",
        "\n",
        "## üå≥ Why Pruning Is Needed\n",
        "\n",
        "Without pruning:\n",
        "\n",
        "* A tree can keep splitting until each **leaf has only 1 sample**\n",
        "* This leads to:\n",
        "\n",
        "  * **High accuracy on training data** ‚úÖ\n",
        "  * **Poor accuracy on new/unseen data** ‚ùå\n",
        "\n",
        "> The tree becomes like a **memorizing overfit student** ‚Äî not a generalizing one\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Types of Pruning\n",
        "\n",
        "### 1. **Pre-Pruning (a.k.a. Early Stopping)**\n",
        "\n",
        "Stop tree growth **before** it becomes too deep:\n",
        "\n",
        "| Parameter           | Meaning                            |\n",
        "| ------------------- | ---------------------------------- |\n",
        "| `max_depth`         | Max depth of the tree              |\n",
        "| `min_samples_split` | Min samples needed to split a node |\n",
        "| `min_samples_leaf`  | Min samples in a leaf node         |\n",
        "| `max_leaf_nodes`    | Limit the number of final outputs  |\n",
        "\n",
        "‚úÖ Most commonly used and built-in in scikit-learn\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Post-Pruning (a.k.a. Cost-Complexity Pruning)**\n",
        "\n",
        "* Let the tree grow fully\n",
        "* Then **cut back unhelpful branches**\n",
        "* Based on a trade-off between **tree complexity vs performance**\n",
        "\n",
        "In scikit-learn:\n",
        "\n",
        "```python\n",
        "DecisionTreeRegressor(ccp_alpha=0.01)\n",
        "```\n",
        "\n",
        "Where:\n",
        "\n",
        "* `ccp_alpha` is the **cost-complexity pruning parameter**\n",
        "* Higher `ccp_alpha` = more pruning\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary\n",
        "\n",
        "| Term             | Explanation                             |\n",
        "| ---------------- | --------------------------------------- |\n",
        "| **Overfitting**  | Tree memorizes noise in training data   |\n",
        "| **Pruning**      | Prevents/corrects overfitting           |\n",
        "| **Pre-pruning**  | Stops splits early (e.g., max\\_depth=4) |\n",
        "| **Post-pruning** | Removes weak branches after training    |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JErhHZ6bnvjU"
      }
    }
  ]
}