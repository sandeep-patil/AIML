{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üå≤ Random Forest Regression\n",
        "\n",
        "### ‚úÖ What Is It?\n",
        "\n",
        "> A **Random Forest** is a group (forest) of **Decision Trees**, each trained on a slightly different version of your dataset.\n",
        "> It averages their predictions to produce a more stable and accurate result.\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Why It Works So Well\n",
        "\n",
        "| Feature               | Benefit                                  |\n",
        "| --------------------- | ---------------------------------------- |\n",
        "| **Multiple trees**    | Reduces overfitting from any single tree |\n",
        "| **Randomness**        | Improves generalization                  |\n",
        "| **Ensemble learning** | Combines weak learners into a strong one |\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Step 2: How It Works Internally\n",
        "\n",
        "1. **Creates many trees** (e.g., 100)\n",
        "2. For each tree:\n",
        "\n",
        "   * Trains on a **random subset of rows** (bootstrap sample)\n",
        "   * Considers only a **random subset of features** at each split\n",
        "3. Each tree gives a prediction\n",
        "4. Final prediction = **average of all tree predictions**\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Step 3: Mathematics Behind It\n",
        "\n",
        "### Each tree:\n",
        "\n",
        "Minimizes **Mean Squared Error (MSE)** as before:\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "### The forest:\n",
        "\n",
        "Combines predictions:\n",
        "\n",
        "$$\n",
        "\\hat{y}_{\\text{final}} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{y}_t\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $T$ = number of trees\n",
        "* $\\hat{y}_t$ = prediction from tree $t$\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ Compared to Decision Tree\n",
        "\n",
        "| Feature          | Decision Tree | Random Forest          |\n",
        "| ---------------- | ------------- | ---------------------- |\n",
        "| Model type       | Single model  | Ensemble of many trees |\n",
        "| Overfitting risk | ‚úÖ High        | ‚ùå Low                  |\n",
        "| Accuracy         | ‚ö†Ô∏è Unstable   | ‚úÖ High                 |\n",
        "| Explainability   | ‚úÖ Easy        | ‚ö†Ô∏è Harder              |\n",
        "| Performance      | Fast to train | Slower but robust      |\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Why It's Great for You (#info + #JD)\n",
        "\n",
        "* **Structured data + mixed feature types** ‚Üí perfect match\n",
        "* Robust for:\n",
        "\n",
        "  * **Price prediction**\n",
        "  * **Warranty estimation**\n",
        "  * **Customer behavior**\n",
        "* Works well with **limited data** and **no feature scaling**\n",
        "* Can extract **feature importances** (which features mattered most)\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hxcg0ODs0QTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üå≤ Advanced Info on Random Forest Regression\n",
        "\n",
        "### üîß 1. **Built-In Pruning (Bagging + Randomization)**\n",
        "\n",
        "Random Forest doesn‚Äôt need traditional **post-pruning** like a single decision tree, because it combats overfitting by:\n",
        "\n",
        "| Mechanism                 | Effect                                   |\n",
        "| ------------------------- | ---------------------------------------- |\n",
        "| **Bootstrap Sampling**    | Trees trained on random data subsets     |\n",
        "| **Random Feature Subset** | Forces trees to learn different patterns |\n",
        "| **Tree Depth Limiting**   | Optional ‚Äî you can still use `max_depth` |\n",
        "| **Averaging Output**      | Smooths predictions, reduces variance    |\n",
        "\n",
        "‚úÖ So pruning = **not needed manually**, but you **can still apply limits** for faster training or less complexity.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç 2. **Key Hyperparameters**\n",
        "\n",
        "| Parameter           | Description                                     |\n",
        "| ------------------- | ----------------------------------------------- |\n",
        "| `n_estimators`      | Number of trees (e.g., 100, 500)                |\n",
        "| `max_depth`         | Limit depth of each tree                        |\n",
        "| `min_samples_split` | Minimum samples needed to split a node          |\n",
        "| `min_samples_leaf`  | Minimum samples in a leaf node                  |\n",
        "| `max_features`      | How many features to consider at each split     |\n",
        "| `bootstrap`         | Whether to bootstrap the samples (True/False)   |\n",
        "| `oob_score`         | Use out-of-bag samples to estimate test score ‚úÖ |\n",
        "\n",
        "‚úÖ For your case (car resale):\n",
        "\n",
        "* `n_estimators=100` is a good starting point\n",
        "* Use `max_depth` or `min_samples_leaf` to reduce overfitting\n",
        "* Enable `oob_score=True` to get a built-in CV estimate\n",
        "\n",
        "---\n",
        "\n",
        "### üìä 3. **Feature Importance**\n",
        "\n",
        "Random Forest can tell you **which features were most useful** across all trees:\n",
        "\n",
        "```python\n",
        "model.feature_importances_\n",
        "```\n",
        "\n",
        "Use it to:\n",
        "\n",
        "* **Drop low-value features** to simplify model\n",
        "* **Explain model behavior** to business/stakeholders\n",
        "* Feed into further **XAI tools like SHAP**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 4. **How It Helps in Automotive Applications (#JD)**\n",
        "\n",
        "| Use Case                        | Why Random Forest Helps                |\n",
        "| ------------------------------- | -------------------------------------- |\n",
        "| Vehicle resale price estimation | Non-linear, mixed features             |\n",
        "| Predictive maintenance          | Robust to noise in sensor logs         |\n",
        "| Warranty cost forecasting       | Captures interactions + outliers       |\n",
        "| Customer segmentation           | Tree logic fits discrete decisions     |\n",
        "| Telematics data usage           | Handles high dimensional, raw features |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è 5. **Comparison with Decision Tree**\n",
        "\n",
        "| Aspect           | Decision Tree         | Random Forest       |\n",
        "| ---------------- | --------------------- | ------------------- |\n",
        "| Overfitting Risk | ‚úÖ High                | ‚ùå Lower (averaging) |\n",
        "| Interpretability | ‚úÖ Visual tree         | ‚ö†Ô∏è Less transparent |\n",
        "| Accuracy         | ‚ö†Ô∏è Depends on pruning | ‚úÖ Usually better    |\n",
        "| Feature Ranking  | ‚ùå None                | ‚úÖ Built-in          |\n",
        "\n",
        "---\n",
        "\n",
        "### üîê 6. **When Not to Use It**\n",
        "\n",
        "* You need **fully interpretable rules**\n",
        "* You‚Äôre dealing with **massive datasets and need very fast inference**\n",
        "* You prefer a **compact model** (e.g., mobile/embedded use)\n",
        "\n",
        "> Tip: Use **Gradient Boosting (like XGBoost)** or **Tree Distillation** if needed later\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0Jj0kjRW1m5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üîß Hyperparameter Comparison: Decision Tree vs Random Forest\n",
        "\n",
        "| Hyperparameter      | Decision Tree                    | Random Forest                             | Effect / Purpose                               |\n",
        "| ------------------- | -------------------------------- | ----------------------------------------- | ---------------------------------------------- |\n",
        "| `criterion`         | `'squared_error'` (default)      | `'squared_error'` (default)               | Loss function to minimize (MSE for regression) |\n",
        "| `max_depth`         | ‚úÖ You control max tree depth     | ‚úÖ Controls depth of each tree             | Prevents overfitting; sets complexity          |\n",
        "| `min_samples_split` | ‚úÖ Min samples to split a node    | ‚úÖ Per tree                                | Higher = less complex, prevents small branches |\n",
        "| `min_samples_leaf`  | ‚úÖ Min samples in a leaf          | ‚úÖ Per tree                                | Enforces minimum leaf size, avoids overfitting |\n",
        "| `max_features`      | ‚ùå Uses all features              | ‚úÖ Defaults to `sqrt(n_features)`          | Adds randomness; improves generalization       |\n",
        "| `max_leaf_nodes`    | ‚úÖ Max number of leaf nodes       | ‚úÖ Per tree                                | Upper bound on tree size                       |\n",
        "| `random_state`      | ‚úÖ Used for reproducibility       | ‚úÖ Also used for bootstrapping             | Fix randomness for reproducible results        |\n",
        "| `splitter`          | `'best'` or `'random'`           | ‚ùå Not applicable                          | Controls how best split is chosen              |\n",
        "| `ccp_alpha`         | ‚úÖ Cost complexity pruning (post) | ‚ùå Rarely used in RF (handled by ensemble) | Prune weak branches post training              |\n",
        "| `n_estimators`      | ‚ùå Only one tree                  | ‚úÖ Number of trees in the forest           | More trees = better stability (but slower)     |\n",
        "| `bootstrap`         | ‚ùå Not used                       | ‚úÖ Default = True                          | Use sampling with replacement per tree         |\n",
        "| `oob_score`         | ‚ùå Not available                  | ‚úÖ Optional (True/False)                   | Estimate test score using unused rows          |\n",
        "| `n_jobs`            | ‚ùå Not needed                     | ‚úÖ Enables parallel training               | `-1` = use all cores                           |\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Key Takeaways:\n",
        "\n",
        "* **Decision Tree** is simple and fast, but prone to overfitting without tuning\n",
        "* **Random Forest** adds **randomness + averaging**, making it **much more robust**\n",
        "* Use **`max_depth`, `min_samples_leaf`** in both to control complexity\n",
        "* Use **`n_estimators`, `max_features`, `bootstrap`** in RF for boosting performance and generalization\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3jEXsYJT2PQ1"
      }
    }
  ]
}