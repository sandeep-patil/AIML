{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression**"
      ],
      "metadata": {
        "id": "kD9LiyhyDeXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† What is Linear Regression?\n",
        "\n",
        "It‚Äôs a method to find the **best straight line** that predicts an output **$y$** from input features **$x_1, x_2, ..., x_n$**.\n",
        "\n",
        "---\n",
        "\n",
        "## üìè Mathematical Equation\n",
        "\n",
        "### üîπ **For one feature (Simple Linear Regression)**\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_1 x + b\n",
        "$$\n",
        "\n",
        "* $\\hat{y}$ ‚Üí predicted output\n",
        "* $x$ ‚Üí input feature\n",
        "* $w_1$ ‚Üí weight (slope of the line)\n",
        "* $b$ ‚Üí bias (intercept ‚Äî where the line cuts the y-axis)\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **For multiple features (Multiple Linear Regression)**\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
        "$$\n",
        "\n",
        "Or using vector notation:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\mathbf{w} = [w_1, w_2, ..., w_n]$\n",
        "* $\\mathbf{x} = [x_1, x_2, ..., x_n]$\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Goal of Linear Regression\n",
        "\n",
        "Find the **best weights $w_1, w_2, ..., w_n$** and **bias $b$** such that the **difference between actual $y$** and **predicted $\\hat{y}$** is minimized.\n",
        "\n",
        "This difference is called the **error**.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Loss Function (What We Minimize)\n",
        "\n",
        "We use **Mean Squared Error (MSE)**:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "* $m$ = number of data points\n",
        "* $y_i$ = actual value\n",
        "* $\\hat{y}_i$ = predicted value using weights and bias\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úèÔ∏è Summary in One Line\n",
        "\n",
        "> Linear Regression finds the best-fitting straight line by minimizing the **average squared difference** between actual and predicted values.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Maths Behind\n",
        "\n",
        "Problem: Predict Marks Based on Study Hours\n",
        "\n",
        "You have the following data:\n",
        "\n",
        "| Hours Studied (x) | Marks Scored (y) |\n",
        "| ----------------- | ---------------- |\n",
        "| 1                 | 2                |\n",
        "| 2                 | 4                |\n",
        "| 3                 | 5                |\n",
        "| 4                 | 4                |\n",
        "| 5                 | 5                |\n",
        "\n",
        "We want to find a straight-line equation:\n",
        "\n",
        "$$\n",
        "\\hat{y} = w x + b\n",
        "$$\n",
        "\n",
        "That predicts marks $\\hat{y}$ based on hours studied $x$.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Step 1: Calculate Averages\n",
        "\n",
        "$$\n",
        "\\bar{x} = \\frac{1 + 2 + 3 + 4 + 5}{5} = 3\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\bar{y} = \\frac{2 + 4 + 5 + 4 + 5}{5} = 4\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Step 2: Calculate Slope $w$\n",
        "\n",
        "Use the formula: The equation shown below is the formula for the slope (w) of the simple linear regression line using the least squares method.\n",
        "\n",
        "$$\n",
        "w = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
        "$$\n",
        "\n",
        "Let‚Äôs compute step-by-step:\n",
        "\n",
        "| x | y | $x - \\bar{x}$ | $y - \\bar{y}$ | $(x - \\bar{x})(y - \\bar{y})$ | $(x - \\bar{x})^2$ |\n",
        "| - | - | ------------- | ------------- | ---------------------------- | ----------------- |\n",
        "| 1 | 2 | -2            | -2            | 4                            | 4                 |\n",
        "| 2 | 4 | -1            | 0             | 0                            | 1                 |\n",
        "| 3 | 5 | 0             | 1             | 0                            | 0                 |\n",
        "| 4 | 4 | 1             | 0             | 0                            | 1                 |\n",
        "| 5 | 5 | 2             | 1             | 2                            | 4                 |\n",
        "|   |   |               |               | **Total: 6**                 | **Total: 10**     |\n",
        "\n",
        "$$\n",
        "w = \\frac{6}{10} = 0.6\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Step 3: Calculate Intercept $b$\n",
        "\n",
        "$$\n",
        "b = \\bar{y} - w \\bar{x} = 4 - 0.6 √ó 3 = 2.2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Final Equation\n",
        "\n",
        "$$\n",
        "\\hat{y} = 0.6x + 2.2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Step 4: Predict\n",
        "\n",
        "If a student studies for 4 hours:\n",
        "\n",
        "$$\n",
        "\\hat{y} = 0.6 √ó 4 + 2.2 = 4.6 \\text{ marks}\n",
        "$$\n",
        "\n",
        "---\n",
        "Great! Let‚Äôs continue with **Step-by-Step Evaluation** of our **Linear Regression model**:\n",
        "\n",
        "We already derived the prediction line:\n",
        "\n",
        "$$\n",
        "\\hat{y} = 0.6x + 2.2\n",
        "$$\n",
        "\n",
        "Let‚Äôs now evaluate this using:\n",
        "\n",
        "1. **Predicted values $\\hat{y}$**\n",
        "2. **Residuals (Errors)**\n",
        "3. **MSE (Mean Squared Error)**\n",
        "4. **R¬≤ (R-squared score)**\n",
        "5. **Plot (visual explanation)**\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ Step 1: Predictions and Residuals\n",
        "\n",
        "| x (Hours) | y (Actual Marks) | $\\hat{y}$ = 0.6x + 2.2 | Residual = $y - \\hat{y}$ |\n",
        "| --------- | ---------------- | ---------------------- | ------------------------ |\n",
        "| 1         | 2                | 0.6√ó1 + 2.2 = 2.8      | -0.8                     |\n",
        "| 2         | 4                | 0.6√ó2 + 2.2 = 3.4      | +0.6                     |\n",
        "| 3         | 5                | 0.6√ó3 + 2.2 = 4.0      | +1.0                     |\n",
        "| 4         | 4                | 0.6√ó4 + 2.2 = 4.6      | -0.6                     |\n",
        "| 5         | 5                | 0.6√ó5 + 2.2 = 5.2      | -0.2                     |\n",
        "\n",
        "---\n",
        "\n",
        "## üìè Step 2: MSE (Mean Squared Error)\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum (y - \\hat{y})^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\frac{(-0.8)^2 + (0.6)^2 + (1)^2 + (-0.6)^2 + (-0.2)^2}{5}\n",
        "= \\frac{0.64 + 0.36 + 1 + 0.36 + 0.04}{5} = \\frac{2.4}{5} = 0.48\n",
        "$$\n",
        "\n",
        "‚úÖ **MSE = 0.48**\n",
        "(Small value = good)\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Step 3: R¬≤ Score (Goodness of Fit)\n",
        "\n",
        "Formula(SS = Sum of Sqaures):\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{SS_\\text{res}}{SS_\\text{tot}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $SS_\\text{res} = \\sum (y - \\hat{y})^2 = 2.4$\n",
        "* $SS_\\text{tot} = \\sum (y - \\bar{y})^2 = (2-4)^2 + (4-4)^2 + (5-4)^2 + (4-4)^2 + (5-4)^2 = 4 + 0 + 1 + 0 + 1 = 6$\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{2.4}{6} = 1 - 0.4 = 0.6\n",
        "$$\n",
        "\n",
        "‚úÖ **R¬≤ = 0.6**\n",
        "This means **60% of the variation in marks is explained** by hours studied.\n",
        "\n",
        "---\n",
        "\n",
        "| R¬≤ Score | How Good is the Model         |\n",
        "| -------- | ----------------------------- |\n",
        "| 1.0      | Perfect! üöÄ                   |\n",
        "| 0.7‚Äì0.9  | Very good üëç                  |\n",
        "| 0.4‚Äì0.6  | Decent üòê                     |\n",
        "| 0‚Äì0.3    | Weak üëé                       |\n",
        "| < 0      | Worse than guessing average ‚ùå |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "29dSa6Ddwfhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ GENERALIZED PROCESS (for both Linear & Lasso Regression):\n",
        "\n",
        "### üîÅ Step-by-Step:\n",
        "\n",
        "1. **Initialize** weights $w_1, w_2, ..., w_n$ and intercept $b$\n",
        "\n",
        "2. **Make Predictions**\n",
        "\n",
        "   $$\n",
        "   \\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
        "   $$\n",
        "\n",
        "3. **Calculate Error**\n",
        "   Usually **Mean Squared Error (MSE)**:\n",
        "\n",
        "   $$\n",
        "   \\text{MSE} = \\frac{1}{m} \\sum (y - \\hat{y})^2\n",
        "   $$\n",
        "\n",
        "4. **(If Lasso) Add Penalty**\n",
        "\n",
        "   $$\n",
        "   \\text{Lasso Loss} = \\text{MSE} + \\alpha \\sum |w_i|\n",
        "   $$\n",
        "\n",
        "5. **Adjust Weights and Bias**\n",
        "   Change weights **to reduce the total loss** (error + penalty in case of Lasso)\n",
        "\n",
        "6. **Repeat Steps 2‚Äì5**\n",
        "   Until the **error is minimized** (or until improvement is very small)\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Key Difference in Step 4:\n",
        "\n",
        "| Type                  | What‚Äôs minimized?                         |\n",
        "| --------------------- | ----------------------------------------- |\n",
        "| **Linear Regression** | Just prediction error (MSE)               |\n",
        "| **Lasso Regression**  | Prediction error **+** penalty on weights |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå So in Simple Words:\n",
        "\n",
        "> Yes ‚Äî in both linear and lasso, the model **guesses weights**, **checks how bad the error is**, and **keeps adjusting them** to make the **total error as small as possible**.\n",
        "> Lasso adds **extra pressure to keep weights small or zero**.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9QVk1Gc76Nwi"
      }
    }
  ]
}