{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Means Clustering**"
      ],
      "metadata": {
        "id": "mJqLSc-bPdWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📌 1. Technical Introduction\n",
        "\n",
        "**Where does K-Means fit?**\n",
        "\n",
        "* Part of **Unsupervised Learning**, specifically under **Clustering Algorithms**.\n",
        "* It groups data into **K distinct clusters** based on similarity.\n",
        "\n",
        "**How does it work conceptually?**\n",
        "\n",
        "1. Choose the number of clusters $K$\n",
        "2. Randomly pick $K$ data points as **initial centroids**\n",
        "3. Assign every data point to the **nearest centroid** (based on Euclidean distance)\n",
        "4. Recalculate centroids as the **mean** of the assigned points\n",
        "5. Repeat steps 3–4 until centroids no longer change\n",
        "\n",
        "**Key Terms**:\n",
        "\n",
        "* **Cluster**: Group of similar points\n",
        "* **Centroid**: Center of a cluster\n",
        "* **Inertia**: Total distance of points from their centroids\n",
        "* **Convergence**: When centroids stop moving significantly\n",
        "\n",
        "---\n",
        "\n",
        "## 🧸 2. Simplified Explanation\n",
        "\n",
        "Imagine you have a **pile of toy cars** of different shapes and colors.\n",
        "\n",
        "You don’t know their categories — but you start **grouping them by similarity** (e.g., all red ones in one place, all trucks in another).\n",
        "\n",
        "That’s what K-Means does — **groups things without knowing their labels**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📕 3. Definition\n",
        "\n",
        "> **K-Means Clustering** is an unsupervised learning algorithm that partitions data into $K$ clusters by minimizing the distance between each point and the centroid of its assigned cluster.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 4. Simple Analogy\n",
        "\n",
        "🎨 **Paint Buckets Analogy**:\n",
        "Imagine you have 100 colors and 3 empty paint buckets.\n",
        "You keep sorting colors into 3 buckets by similarity until each bucket has its \"theme.\"\n",
        "Eventually, each color is in the bucket that’s “closest” to it — that's K-Means!\n",
        "\n",
        "---\n",
        "\n",
        "## 🚗 5. Examples\n",
        "\n",
        "### 🚘 Automotive Use Case:\n",
        "\n",
        "* **Vehicle Behavior Clustering**: Group driver behavior (aggressive, normal, cautious) based on speed, braking, acceleration patterns.\n",
        "* **Maintenance Clustering**: Group engine signals into normal, degraded, or fault states.\n",
        "* **Road Condition Detection**: Use vibration sensor data to cluster smooth vs bumpy roads.\n",
        "\n",
        "### 🌍 General Use Cases:\n",
        "\n",
        "* **Customer Segmentation**\n",
        "* **Image Compression**\n",
        "* **Document Clustering**\n",
        "* **Anomaly Detection**\n",
        "\n",
        "---\n",
        "\n",
        "## 📐 6. Mathematical Equations\n",
        "\n",
        "Let:\n",
        "\n",
        "* $X = \\{x_1, x_2, ..., x_n\\}$: dataset\n",
        "* $\\mu_k$: centroid of cluster $k$\n",
        "\n",
        "### Objective Function:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i=1}^{n} \\sum_{k=1}^{K} w_{ik} \\cdot \\|x_i - \\mu_k\\|^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $w_{ik} = 1$ if $x_i$ belongs to cluster $k$, else 0\n",
        "* $\\|x_i - \\mu_k\\|^2$ is squared Euclidean distance\n",
        "\n",
        "Goal: **Minimize J** by adjusting centroids and point assignments.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 7. Important Information\n",
        "\n",
        "* K-Means assumes **spherical, equally sized clusters**\n",
        "* It’s **sensitive to scale** — always standardize or normalize features\n",
        "* The final result depends on **initial centroid positions**\n",
        "* Doesn’t work well with **non-convex shapes** or **varying cluster sizes**\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 8. Comparison with Similar Methods\n",
        "\n",
        "| Feature              | K-Means     | Hierarchical  | DBSCAN       |\n",
        "| -------------------- | ----------- | ------------- | ------------ |\n",
        "| Requires `K` upfront | ✅ Yes       | ❌ No          | ❌ No         |\n",
        "| Handles noise        | ❌ No        | ❌ No          | ✅ Yes        |\n",
        "| Shape of clusters    | 🔵 Circular | 🔗 Tree-based | 🌐 Arbitrary |\n",
        "| Speed                | ⚡ Fast      | 🐢 Slower     | ⚠ Medium     |\n",
        "| Scalability          | ✅ High      | ❌ Low         | ⚠ Medium     |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 9. Advantages and Disadvantages\n",
        "\n",
        "### ✅ Advantages:\n",
        "\n",
        "* Simple and fast for large datasets\n",
        "* Works well when clusters are clearly separated\n",
        "* Easy to implement and interpret\n",
        "\n",
        "### ❌ Disadvantages:\n",
        "\n",
        "* Must specify `K` beforehand\n",
        "* Sensitive to initial centroid choice\n",
        "* Poor performance with non-spherical clusters or noise\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ 10. Things to Watch Out For\n",
        "\n",
        "* **Always scale data** before applying K-Means\n",
        "* **Use Elbow Method or Silhouette Score to find best `K`**\n",
        "* Run K-Means **multiple times** with different initializations (`k-means++` helps)\n",
        "* Doesn’t work well with **categorical data**\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 11. Other Critical Insights\n",
        "\n",
        "* PCA is often used **before K-Means** to reduce dimensionality and improve clustering\n",
        "* Combine with **DBSCAN** for complex patterns\n",
        "* For **streaming data**, use **MiniBatch K-Means**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "EF6jO1t3Pfd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MiniBatch K-Means**"
      ],
      "metadata": {
        "id": "SZBQg0DsUwBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 📌 1. Technical Introduction\n",
        "\n",
        "### 🧭 Where It Fits:\n",
        "\n",
        "* It's part of **Unsupervised Learning**, under **Clustering**\n",
        "* It is a **faster, more scalable version** of K-Means\n",
        "* Used when:\n",
        "\n",
        "  * Data is too large for standard K-Means\n",
        "  * You want **real-time clustering** or **streamed input**\n",
        "\n",
        "### 🛠 How It Works Conceptually:\n",
        "\n",
        "* Instead of using **all data points** at each iteration to update centroids, it uses a **random mini-batch (subset)**.\n",
        "* This makes it **faster and memory-efficient**, especially for large datasets.\n",
        "\n",
        "### Key Terms:\n",
        "\n",
        "* **Mini-batch**: A small, random subset of the data\n",
        "* **Partial update**: Centroids are updated using only the batch, not the whole dataset\n",
        "* **Stochastic approximation**: Updates are noisy, but converge\n",
        "\n",
        "---\n",
        "\n",
        "## 🧸 2. Simplified Explanation (No Jargon)\n",
        "\n",
        "Imagine trying to sort 1 lakh toy cars into 5 groups — it's slow if you sort them all at once.\n",
        "\n",
        "MiniBatch K-Means says:\n",
        "\n",
        "> “Let’s take just 100 cars at a time, sort them quickly, and adjust our idea of the groups a little each time.”\n",
        "\n",
        "Repeat this many times, and you still end up with great clusters — **faster and lighter**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📕 3. Definition\n",
        "\n",
        "> **MiniBatch K-Means** is a faster variation of K-Means that updates cluster centroids using small random subsets (mini-batches) of the data, making it more efficient for large-scale datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 4. Simple Analogy\n",
        "\n",
        "🛍️ **Grocery Store Sorting Analogy**:\n",
        "Instead of sorting all 10,000 items at once into shelves, you take 50 items at a time, arrange them, and adjust shelf categories slightly.\n",
        "Keep doing that — and shelves slowly improve their organization over time.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚗 5. Examples\n",
        "\n",
        "### 🚘 Automotive:\n",
        "\n",
        "* **Real-time vehicle telemetry clustering** (e.g., classify driving styles from live streaming data)\n",
        "* **Live traffic clustering** from vehicle-to-vehicle (V2V) communication\n",
        "* **Edge devices** clustering sensor data with limited memory\n",
        "\n",
        "### 🌍 General:\n",
        "\n",
        "* Clustering **millions of online transactions**\n",
        "* Organizing images or documents in real-time\n",
        "* Grouping user behavior on large websites\n",
        "\n",
        "---\n",
        "\n",
        "## 📐 6. Mathematical Core\n",
        "\n",
        "MiniBatch K-Means tries to **minimize the same cost** as K-Means:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i=1}^{n} \\sum_{k=1}^{K} w_{ik} \\cdot \\|x_i - \\mu_k\\|^2\n",
        "$$\n",
        "\n",
        "### What’s different?\n",
        "\n",
        "* Instead of all $n$ points, it updates using just a **mini-batch of size `b`** per iteration.\n",
        "\n",
        "Centroid update:\n",
        "\n",
        "$$\n",
        "\\mu_k^{(t+1)} = \\mu_k^{(t)} + \\alpha \\cdot (x_i - \\mu_k^{(t)})\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\alpha$: Learning rate (decay with iterations)\n",
        "* $x_i$: Random sample from the batch assigned to cluster $k$\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 7. Important Information\n",
        "\n",
        "* **Doesn't always converge to exact K-Means result** — it’s an approximation\n",
        "* Needs **random shuffling** to be effective\n",
        "* Still requires `k` (number of clusters) to be defined\n",
        "* **Much faster** on large datasets\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 8. Comparison Table\n",
        "\n",
        "| Feature                | K-Means        | MiniBatch K-Means        |\n",
        "| ---------------------- | -------------- | ------------------------ |\n",
        "| Uses all data per step | ✅ Yes          | ❌ No (uses mini-batch)   |\n",
        "| Speed                  | ⚠ Slower       | ✅ Much faster            |\n",
        "| Memory Usage           | High           | Low                      |\n",
        "| Accuracy               | High           | Slightly lower (approx.) |\n",
        "| Best for               | Small datasets | Large/streaming datasets |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 9. Advantages and Disadvantages\n",
        "\n",
        "### ✅ Advantages:\n",
        "\n",
        "* 🚀 Faster for large datasets\n",
        "* 💾 Works with limited memory\n",
        "* 🔁 Suitable for real-time clustering\n",
        "* 🧠 Still finds useful clusters\n",
        "\n",
        "### ❌ Disadvantages:\n",
        "\n",
        "* 🧭 Results may vary per run (less stable)\n",
        "* 📉 Slight drop in accuracy vs full K-Means\n",
        "* ⚙ Needs tuning for mini-batch size\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ 10. Things to Watch Out For\n",
        "\n",
        "* Pick a good **mini-batch size** (e.g., 10% of dataset, or \\~100–1000)\n",
        "* Still scale your features\n",
        "* Run multiple times and average results if consistency is needed\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 11. Other Critical Insights\n",
        "\n",
        "* Available in **scikit-learn** as `MiniBatchKMeans`\n",
        "* You can track **inertia** (cost) across iterations to monitor convergence\n",
        "* Use with **PCA** before clustering if data is high-dimensional\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uauKNOiWU1CD"
      }
    }
  ]
}