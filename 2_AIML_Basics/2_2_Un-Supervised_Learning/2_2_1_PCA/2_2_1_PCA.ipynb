{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA (Principal Component Analysis)**\n",
        "\n",
        "* It transforms a high-dimensional dataset into a lower-dimensional one without losing much information.\n",
        "\n",
        "* PCA is a technique in Unsupervised Learning, specifically under Dimensionality Reduction."
      ],
      "metadata": {
        "id": "lh6fOCLTJHno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does PCA do?\n",
        "\n",
        "* PCA finds new axes (principal components) that explain maximum variance in the data.\n",
        "\n",
        "* It is used before applying supervised or unsupervised learning — preprocessing step."
      ],
      "metadata": {
        "id": "JLP9Ch4vMs5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a full **#Explain PCA (Principal Component Analysis)** using your custom configuration:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Key Terms:**\n",
        "\n",
        "* **Variance**: How spread out the data is.\n",
        "* **Eigenvectors & Eigenvalues**: Mathematical tools used to find new axes.\n",
        "* **Dimensionality Reduction**: Compressing data by removing less important features.\n",
        "* **Orthogonal**: New axes (principal components) are uncorrelated and at 90° angles to each other.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧸  Simplified Explanation (No-Jargon)\n",
        "\n",
        "Imagine you have **100 photos of cars**, and each photo has **1,000 different features** (pixels/colors/angles).\n",
        "Do you really need all 1,000 to tell the car’s direction? Probably not.\n",
        "\n",
        "PCA looks at all features and says:\n",
        "\n",
        "> \"Hey, these 10 features explain most of the differences. Let’s just keep them and ignore the rest.\"\n",
        "\n",
        "It’s like **shrinking your data smartly**, so your ML model can learn faster and better.\n",
        "\n",
        "---\n",
        "\n",
        "## 📕 Definition\n",
        "\n",
        "> **Principal Component Analysis (PCA)** is an unsupervised linear transformation technique that projects high-dimensional data into a lower-dimensional space by identifying new orthogonal axes (principal components) that capture the most variance.\n",
        "\n",
        "---\n",
        "## 🚗 Examples\n",
        "\n",
        "### 🏎️ Automotive:\n",
        "\n",
        "* **Sensor Fusion**: You collect LIDAR, camera, and radar data. PCA helps reduce redundancy before feeding into a neural network.\n",
        "* **Vehicle Diagnostics**: Reduce hundreds of sensor features into 3–5 main signals that represent the system’s state.\n",
        "\n",
        "### 🌍 General:\n",
        "\n",
        "* **Facial Recognition**: Reduce image pixel data to key facial features.\n",
        "* **Marketing**: Simplify customer data (age, income, spending) into behavior patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## 📐 Mathematical Equations\n",
        "\n",
        "Let’s say you have a dataset **X** with `n` samples and `d` features.\n",
        "\n",
        "### Step-by-step:\n",
        "\n",
        "1. **Standardize the data**:\n",
        "   $X' = \\frac{X - \\mu}{\\sigma}$\n",
        "   (mean = 0, std dev = 1)\n",
        "\n",
        "2. **Covariance matrix**:\n",
        "   $\\Sigma = \\frac{1}{n-1} X'^{T} X'$\n",
        "\n",
        "3. **Eigen-decomposition**:\n",
        "   Find eigenvectors $v_i$ and eigenvalues $\\lambda_i$ of $\\Sigma$\n",
        "\n",
        "4. **Select top `k` components**:\n",
        "   Choose k eigenvectors with highest eigenvalues.\n",
        "\n",
        "5. **Transform data**:\n",
        "   $Z = X' \\cdot V_k$\n",
        "   where $V_k$ is the matrix of top-k eigenvectors.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Important Information\n",
        "\n",
        "* PCA assumes **linearity** and that directions with higher variance are more “informative”.\n",
        "* It is **sensitive to scale**, so **standardize your data** before applying PCA.\n",
        "* Results are **not interpretable** like original features — axes are combinations.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Comparison Table\n",
        "\n",
        "| Feature            | PCA                          | K-Means                   | LDA (Linear Disc. Analysis) |\n",
        "| ------------------ | ---------------------------- | ------------------------- | --------------------------- |\n",
        "| Type               | Unsupervised (Preprocessing) | Unsupervised (Clustering) | Supervised (Reduction)      |\n",
        "| Goal               | Reduce dimensions            | Group similar data        | Separate known classes      |\n",
        "| Works on Labels?   | ❌ No                         | ❌ No                      | ✅ Yes                       |\n",
        "| Based on Variance? | ✅ Yes                        | ❌ No                      | ❌ No (Based on class)       |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 9. Advantages and Disadvantages\n",
        "\n",
        "**✅ Advantages:**\n",
        "\n",
        "* Speeds up ML training by reducing features\n",
        "* Removes multicollinearity (correlated features)\n",
        "* Helps with visualization (2D/3D plots)\n",
        "\n",
        "**❌ Disadvantages:**\n",
        "\n",
        "* Loss of interpretability\n",
        "* Doesn’t work well with non-linear relationships\n",
        "* Sensitive to feature scaling\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ 10. Things to Watch Out For\n",
        "\n",
        "* **Always scale/normalize** before applying PCA.\n",
        "* Don’t choose too many components — defeats the purpose.\n",
        "* Eigenvalue drops rapidly → \"elbow method\" helps pick ideal `k`.\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 11. Other Critical Insights\n",
        "\n",
        "* PCA is often used **before clustering (e.g., K-Means)** to clean noisy dimensions.\n",
        "* Use **Scree Plot** or **Explained Variance Ratio** to decide how many components to keep.\n",
        "* In high-dimensional data (e.g., image processing), PCA is a **must-have step** to reduce cost.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KyWPxaxXM6te"
      }
    }
  ]
}