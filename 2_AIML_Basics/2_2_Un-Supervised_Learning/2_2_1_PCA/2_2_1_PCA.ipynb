{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA (Principal Component Analysis)**\n",
        "\n",
        "* It transforms a high-dimensional dataset into a lower-dimensional one without losing much information.\n",
        "\n",
        "* PCA is a technique in Unsupervised Learning, specifically under Dimensionality Reduction."
      ],
      "metadata": {
        "id": "lh6fOCLTJHno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does PCA do?\n",
        "\n",
        "* PCA finds new axes (principal components) that explain maximum variance in the data.\n",
        "\n",
        "* It is used before applying supervised or unsupervised learning â€” preprocessing step."
      ],
      "metadata": {
        "id": "JLP9Ch4vMs5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hereâ€™s a full **#Explain PCA (Principal Component Analysis)** using your custom configuration:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Key Terms:**\n",
        "\n",
        "* **Variance**: How spread out the data is.\n",
        "* **Eigenvectors & Eigenvalues**: Mathematical tools used to find new axes.\n",
        "* **Dimensionality Reduction**: Compressing data by removing less important features.\n",
        "* **Orthogonal**: New axes (principal components) are uncorrelated and at 90Â° angles to each other.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§¸  Simplified Explanation (No-Jargon)\n",
        "\n",
        "Imagine you have **100 photos of cars**, and each photo has **1,000 different features** (pixels/colors/angles).\n",
        "Do you really need all 1,000 to tell the carâ€™s direction? Probably not.\n",
        "\n",
        "PCA looks at all features and says:\n",
        "\n",
        "> \"Hey, these 10 features explain most of the differences. Letâ€™s just keep them and ignore the rest.\"\n",
        "\n",
        "Itâ€™s like **shrinking your data smartly**, so your ML model can learn faster and better.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“• Definition\n",
        "\n",
        "> **Principal Component Analysis (PCA)** is an unsupervised linear transformation technique that projects high-dimensional data into a lower-dimensional space by identifying new orthogonal axes (principal components) that capture the most variance.\n",
        "\n",
        "---\n",
        "## ğŸš— Examples\n",
        "\n",
        "### ğŸï¸ Automotive:\n",
        "\n",
        "* **Sensor Fusion**: You collect LIDAR, camera, and radar data. PCA helps reduce redundancy before feeding into a neural network.\n",
        "* **Vehicle Diagnostics**: Reduce hundreds of sensor features into 3â€“5 main signals that represent the systemâ€™s state.\n",
        "\n",
        "### ğŸŒ General:\n",
        "\n",
        "* **Facial Recognition**: Reduce image pixel data to key facial features.\n",
        "* **Marketing**: Simplify customer data (age, income, spending) into behavior patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“ Mathematical Equations\n",
        "\n",
        "Letâ€™s say you have a dataset **X** with `n` samples and `d` features.\n",
        "\n",
        "### Step-by-step:\n",
        "\n",
        "1. **Standardize the data**:\n",
        "   $X' = \\frac{X - \\mu}{\\sigma}$\n",
        "   (mean = 0, std dev = 1)\n",
        "\n",
        "2. **Covariance matrix**:\n",
        "   $\\Sigma = \\frac{1}{n-1} X'^{T} X'$\n",
        "\n",
        "3. **Eigen-decomposition**:\n",
        "   Find eigenvectors $v_i$ and eigenvalues $\\lambda_i$ of $\\Sigma$\n",
        "\n",
        "4. **Select top `k` components**:\n",
        "   Choose k eigenvectors with highest eigenvalues.\n",
        "\n",
        "5. **Transform data**:\n",
        "   $Z = X' \\cdot V_k$\n",
        "   where $V_k$ is the matrix of top-k eigenvectors.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Œ Important Information\n",
        "\n",
        "* PCA assumes **linearity** and that directions with higher variance are more â€œinformativeâ€.\n",
        "* It is **sensitive to scale**, so **standardize your data** before applying PCA.\n",
        "* Results are **not interpretable** like original features â€” axes are combinations.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” Comparison Table\n",
        "\n",
        "| Feature            | PCA                          | K-Means                   | LDA (Linear Disc. Analysis) |\n",
        "| ------------------ | ---------------------------- | ------------------------- | --------------------------- |\n",
        "| Type               | Unsupervised (Preprocessing) | Unsupervised (Clustering) | Supervised (Reduction)      |\n",
        "| Goal               | Reduce dimensions            | Group similar data        | Separate known classes      |\n",
        "| Works on Labels?   | âŒ No                         | âŒ No                      | âœ… Yes                       |\n",
        "| Based on Variance? | âœ… Yes                        | âŒ No                      | âŒ No (Based on class)       |\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… 9. Advantages and Disadvantages\n",
        "\n",
        "**âœ… Advantages:**\n",
        "\n",
        "* Speeds up ML training by reducing features\n",
        "* Removes multicollinearity (correlated features)\n",
        "* Helps with visualization (2D/3D plots)\n",
        "\n",
        "**âŒ Disadvantages:**\n",
        "\n",
        "* Loss of interpretability\n",
        "* Doesnâ€™t work well with non-linear relationships\n",
        "* Sensitive to feature scaling\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ 10. Things to Watch Out For\n",
        "\n",
        "* **Always scale/normalize** before applying PCA.\n",
        "* Donâ€™t choose too many components â€” defeats the purpose.\n",
        "* Eigenvalue drops rapidly â†’ \"elbow method\" helps pick ideal `k`.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ 11. Other Critical Insights\n",
        "\n",
        "* PCA is often used **before clustering (e.g., K-Means)** to clean noisy dimensions.\n",
        "* Use **Scree Plot** or **Explained Variance Ratio** to decide how many components to keep.\n",
        "* In high-dimensional data (e.g., image processing), PCA is a **must-have step** to reduce cost.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KyWPxaxXM6te"
      }
    }
  ]
}