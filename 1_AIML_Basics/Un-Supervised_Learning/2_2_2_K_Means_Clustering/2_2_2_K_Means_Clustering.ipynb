{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **K-Means Clustering**"
      ],
      "metadata": {
        "id": "mJqLSc-bPdWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Œ 1. Technical Introduction\n",
        "\n",
        "**Where does K-Means fit?**\n",
        "\n",
        "* Part of **Unsupervised Learning**, specifically under **Clustering Algorithms**.\n",
        "* It groups data into **K distinct clusters** based on similarity.\n",
        "\n",
        "**How does it work conceptually?**\n",
        "\n",
        "1. Choose the number of clusters $K$\n",
        "2. Randomly pick $K$ data points as **initial centroids**\n",
        "3. Assign every data point to the **nearest centroid** (based on Euclidean distance)\n",
        "4. Recalculate centroids as the **mean** of the assigned points\n",
        "5. Repeat steps 3â€“4 until centroids no longer change\n",
        "\n",
        "**Key Terms**:\n",
        "\n",
        "* **Cluster**: Group of similar points\n",
        "* **Centroid**: Center of a cluster\n",
        "* **Inertia**: Total distance of points from their centroids\n",
        "* **Convergence**: When centroids stop moving significantly\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§¸ 2. Simplified Explanation\n",
        "\n",
        "Imagine you have a **pile of toy cars** of different shapes and colors.\n",
        "\n",
        "You donâ€™t know their categories â€” but you start **grouping them by similarity** (e.g., all red ones in one place, all trucks in another).\n",
        "\n",
        "Thatâ€™s what K-Means does â€” **groups things without knowing their labels**.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“• 3. Definition\n",
        "\n",
        "> **K-Means Clustering** is an unsupervised learning algorithm that partitions data into $K$ clusters by minimizing the distance between each point and the centroid of its assigned cluster.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§  4. Simple Analogy\n",
        "\n",
        "ğŸ¨ **Paint Buckets Analogy**:\n",
        "Imagine you have 100 colors and 3 empty paint buckets.\n",
        "You keep sorting colors into 3 buckets by similarity until each bucket has its \"theme.\"\n",
        "Eventually, each color is in the bucket thatâ€™s â€œclosestâ€ to it â€” that's K-Means!\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš— 5. Examples\n",
        "\n",
        "### ğŸš˜ Automotive Use Case:\n",
        "\n",
        "* **Vehicle Behavior Clustering**: Group driver behavior (aggressive, normal, cautious) based on speed, braking, acceleration patterns.\n",
        "* **Maintenance Clustering**: Group engine signals into normal, degraded, or fault states.\n",
        "* **Road Condition Detection**: Use vibration sensor data to cluster smooth vs bumpy roads.\n",
        "\n",
        "### ğŸŒ General Use Cases:\n",
        "\n",
        "* **Customer Segmentation**\n",
        "* **Image Compression**\n",
        "* **Document Clustering**\n",
        "* **Anomaly Detection**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“ 6. Mathematical Equations\n",
        "\n",
        "Let:\n",
        "\n",
        "* $X = \\{x_1, x_2, ..., x_n\\}$: dataset\n",
        "* $\\mu_k$: centroid of cluster $k$\n",
        "\n",
        "### Objective Function:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i=1}^{n} \\sum_{k=1}^{K} w_{ik} \\cdot \\|x_i - \\mu_k\\|^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $w_{ik} = 1$ if $x_i$ belongs to cluster $k$, else 0\n",
        "* $\\|x_i - \\mu_k\\|^2$ is squared Euclidean distance\n",
        "\n",
        "Goal: **Minimize J** by adjusting centroids and point assignments.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Œ 7. Important Information\n",
        "\n",
        "* K-Means assumes **spherical, equally sized clusters**\n",
        "* Itâ€™s **sensitive to scale** â€” always standardize or normalize features\n",
        "* The final result depends on **initial centroid positions**\n",
        "* Doesnâ€™t work well with **non-convex shapes** or **varying cluster sizes**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” 8. Comparison with Similar Methods\n",
        "\n",
        "| Feature              | K-Means     | Hierarchical  | DBSCAN       |\n",
        "| -------------------- | ----------- | ------------- | ------------ |\n",
        "| Requires `K` upfront | âœ… Yes       | âŒ No          | âŒ No         |\n",
        "| Handles noise        | âŒ No        | âŒ No          | âœ… Yes        |\n",
        "| Shape of clusters    | ğŸ”µ Circular | ğŸ”— Tree-based | ğŸŒ Arbitrary |\n",
        "| Speed                | âš¡ Fast      | ğŸ¢ Slower     | âš  Medium     |\n",
        "| Scalability          | âœ… High      | âŒ Low         | âš  Medium     |\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… 9. Advantages and Disadvantages\n",
        "\n",
        "### âœ… Advantages:\n",
        "\n",
        "* Simple and fast for large datasets\n",
        "* Works well when clusters are clearly separated\n",
        "* Easy to implement and interpret\n",
        "\n",
        "### âŒ Disadvantages:\n",
        "\n",
        "* Must specify `K` beforehand\n",
        "* Sensitive to initial centroid choice\n",
        "* Poor performance with non-spherical clusters or noise\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ 10. Things to Watch Out For\n",
        "\n",
        "* **Always scale data** before applying K-Means\n",
        "* **Use Elbow Method or Silhouette Score to find best `K`**\n",
        "* Run K-Means **multiple times** with different initializations (`k-means++` helps)\n",
        "* Doesnâ€™t work well with **categorical data**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ 11. Other Critical Insights\n",
        "\n",
        "* PCA is often used **before K-Means** to reduce dimensionality and improve clustering\n",
        "* Combine with **DBSCAN** for complex patterns\n",
        "* For **streaming data**, use **MiniBatch K-Means**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "EF6jO1t3Pfd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MiniBatch K-Means**"
      ],
      "metadata": {
        "id": "SZBQg0DsUwBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ“Œ 1. Technical Introduction\n",
        "\n",
        "### ğŸ§­ Where It Fits:\n",
        "\n",
        "* It's part of **Unsupervised Learning**, under **Clustering**\n",
        "* It is a **faster, more scalable version** of K-Means\n",
        "* Used when:\n",
        "\n",
        "  * Data is too large for standard K-Means\n",
        "  * You want **real-time clustering** or **streamed input**\n",
        "\n",
        "### ğŸ›  How It Works Conceptually:\n",
        "\n",
        "* Instead of using **all data points** at each iteration to update centroids, it uses a **random mini-batch (subset)**.\n",
        "* This makes it **faster and memory-efficient**, especially for large datasets.\n",
        "\n",
        "### Key Terms:\n",
        "\n",
        "* **Mini-batch**: A small, random subset of the data\n",
        "* **Partial update**: Centroids are updated using only the batch, not the whole dataset\n",
        "* **Stochastic approximation**: Updates are noisy, but converge\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§¸ 2. Simplified Explanation (No Jargon)\n",
        "\n",
        "Imagine trying to sort 1 lakh toy cars into 5 groups â€” it's slow if you sort them all at once.\n",
        "\n",
        "MiniBatch K-Means says:\n",
        "\n",
        "> â€œLetâ€™s take just 100 cars at a time, sort them quickly, and adjust our idea of the groups a little each time.â€\n",
        "\n",
        "Repeat this many times, and you still end up with great clusters â€” **faster and lighter**.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“• 3. Definition\n",
        "\n",
        "> **MiniBatch K-Means** is a faster variation of K-Means that updates cluster centroids using small random subsets (mini-batches) of the data, making it more efficient for large-scale datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§  4. Simple Analogy\n",
        "\n",
        "ğŸ›ï¸ **Grocery Store Sorting Analogy**:\n",
        "Instead of sorting all 10,000 items at once into shelves, you take 50 items at a time, arrange them, and adjust shelf categories slightly.\n",
        "Keep doing that â€” and shelves slowly improve their organization over time.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸš— 5. Examples\n",
        "\n",
        "### ğŸš˜ Automotive:\n",
        "\n",
        "* **Real-time vehicle telemetry clustering** (e.g., classify driving styles from live streaming data)\n",
        "* **Live traffic clustering** from vehicle-to-vehicle (V2V) communication\n",
        "* **Edge devices** clustering sensor data with limited memory\n",
        "\n",
        "### ğŸŒ General:\n",
        "\n",
        "* Clustering **millions of online transactions**\n",
        "* Organizing images or documents in real-time\n",
        "* Grouping user behavior on large websites\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“ 6. Mathematical Core\n",
        "\n",
        "MiniBatch K-Means tries to **minimize the same cost** as K-Means:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i=1}^{n} \\sum_{k=1}^{K} w_{ik} \\cdot \\|x_i - \\mu_k\\|^2\n",
        "$$\n",
        "\n",
        "### Whatâ€™s different?\n",
        "\n",
        "* Instead of all $n$ points, it updates using just a **mini-batch of size `b`** per iteration.\n",
        "\n",
        "Centroid update:\n",
        "\n",
        "$$\n",
        "\\mu_k^{(t+1)} = \\mu_k^{(t)} + \\alpha \\cdot (x_i - \\mu_k^{(t)})\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\alpha$: Learning rate (decay with iterations)\n",
        "* $x_i$: Random sample from the batch assigned to cluster $k$\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Œ 7. Important Information\n",
        "\n",
        "* **Doesn't always converge to exact K-Means result** â€” itâ€™s an approximation\n",
        "* Needs **random shuffling** to be effective\n",
        "* Still requires `k` (number of clusters) to be defined\n",
        "* **Much faster** on large datasets\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” 8. Comparison Table\n",
        "\n",
        "| Feature                | K-Means        | MiniBatch K-Means        |\n",
        "| ---------------------- | -------------- | ------------------------ |\n",
        "| Uses all data per step | âœ… Yes          | âŒ No (uses mini-batch)   |\n",
        "| Speed                  | âš  Slower       | âœ… Much faster            |\n",
        "| Memory Usage           | High           | Low                      |\n",
        "| Accuracy               | High           | Slightly lower (approx.) |\n",
        "| Best for               | Small datasets | Large/streaming datasets |\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… 9. Advantages and Disadvantages\n",
        "\n",
        "### âœ… Advantages:\n",
        "\n",
        "* ğŸš€ Faster for large datasets\n",
        "* ğŸ’¾ Works with limited memory\n",
        "* ğŸ” Suitable for real-time clustering\n",
        "* ğŸ§  Still finds useful clusters\n",
        "\n",
        "### âŒ Disadvantages:\n",
        "\n",
        "* ğŸ§­ Results may vary per run (less stable)\n",
        "* ğŸ“‰ Slight drop in accuracy vs full K-Means\n",
        "* âš™ Needs tuning for mini-batch size\n",
        "\n",
        "---\n",
        "\n",
        "## âš ï¸ 10. Things to Watch Out For\n",
        "\n",
        "* Pick a good **mini-batch size** (e.g., 10% of dataset, or \\~100â€“1000)\n",
        "* Still scale your features\n",
        "* Run multiple times and average results if consistency is needed\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ 11. Other Critical Insights\n",
        "\n",
        "* Available in **scikit-learn** as `MiniBatchKMeans`\n",
        "* You can track **inertia** (cost) across iterations to monitor convergence\n",
        "* Use with **PCA** before clustering if data is high-dimensional\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uauKNOiWU1CD"
      }
    }
  ]
}