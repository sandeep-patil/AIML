{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Boosting Regression**"
      ],
      "metadata": {
        "id": "WF9P1xJvEtql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üî∂ 1. **What is Gradient Boosting in ML?**\n",
        "\n",
        "It is a **supervised learning technique** used for both **regression and classification** problems.\n",
        "It builds a **strong predictive model** by **combining many weak learners**, typically **decision trees**.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∂ 2. **Core Idea**\n",
        "\n",
        "Each tree in the sequence tries to **fix the errors (residuals)** made by the **previous trees**.\n",
        "\n",
        "It does this by:\n",
        "\n",
        "* Predicting how much error is still left (called the **residual**).\n",
        "* Training the next tree on that residual.\n",
        "* Repeating this process and adding each tree's correction to the previous model.\n",
        "\n",
        "---\n",
        "\n",
        "## üî∂ 3. **Algorithm Intuition**\n",
        "\n",
        "Let‚Äôs say your goal is to predict `y` using input features `X`.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "#### Step 1: Start with a **simple model**\n",
        "\n",
        "* Predict all `y` values with a constant. Usually, this is the **mean of y**.\n",
        "\n",
        "#### Step 2: Calculate **residuals** (errors)\n",
        "\n",
        "* Residual = actual `y` - predicted `y`\n",
        "\n",
        "#### Step 3: Train a **small decision tree** to predict the **residuals**.\n",
        "\n",
        "#### Step 4: Add this tree's predictions (corrections) to the current model.\n",
        "\n",
        "#### Step 5: Repeat steps 2‚Äì4 for a number of iterations (or until errors are small).\n",
        "\n",
        "---\n",
        "\n",
        "## üî∂ 4. **Mathematics Behind Gradient Boosting**\n",
        "\n",
        "We want to minimize a **loss function** (like MSE for regression):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(y, F(x)) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - F(x_i))^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y_i$: true value\n",
        "* $F(x_i)$: model‚Äôs prediction\n",
        "\n",
        "### Key steps:\n",
        "\n",
        "#### Step 1: Initialize model with a constant prediction\n",
        "\n",
        "$$\n",
        "F_0(x) = \\arg\\min_\\gamma \\sum_{i=1}^{n} \\mathcal{L}(y_i, \\gamma)\n",
        "$$\n",
        "\n",
        "For MSE, $F_0(x) = \\text{mean}(y)$\n",
        "\n",
        "#### Step 2: For **m = 1 to M (number of trees)**\n",
        "\n",
        "1. Compute **pseudo-residuals** (gradients of the loss function):\n",
        "\n",
        "   $$\n",
        "   r_{im} = -\\left[ \\frac{\\partial \\mathcal{L}(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x)=F_{m-1}(x)}\n",
        "   $$\n",
        "\n",
        "   For MSE, this becomes:\n",
        "\n",
        "   $$\n",
        "   r_{im} = y_i - F_{m-1}(x_i)\n",
        "   $$\n",
        "\n",
        "2. Fit a new tree $h_m(x)$ to the residuals $r_{im}$\n",
        "\n",
        "3. Compute step size $\\gamma_m$:\n",
        "\n",
        "   $$\n",
        "   \\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{n} \\mathcal{L}(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i))\n",
        "   $$\n",
        "\n",
        "4. Update model:\n",
        "\n",
        "   $$\n",
        "   F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "## üî∂ 5. Key Hyperparameters in `GradientBoostingRegressor`\n",
        "\n",
        "* `n_estimators`: Number of trees (iterations)\n",
        "* `learning_rate`: Controls step size $\\gamma_m$\n",
        "* `max_depth`: Controls complexity of each tree\n",
        "* `subsample`: Use fraction of data (for stochastic GB)\n",
        "* `loss`: Loss function to minimize (e.g., MSE)\n",
        "\n",
        "---\n",
        "\n",
        "## üî∂ 6. Advantages\n",
        "\n",
        "‚úÖ High accuracy\n",
        "‚úÖ Handles non-linear relationships\n",
        "‚úÖ Built-in feature importance\n",
        "‚úÖ Robust to overfitting (if tuned properly)\n",
        "\n",
        "---\n",
        "\n",
        "## üî∂ 7. Limitations\n",
        "\n",
        "‚ùå Slower to train than Random Forests\n",
        "‚ùå Sensitive to noisy data\n",
        "‚ùå Needs careful tuning of learning rate & number of trees\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fLA6EnHMEv62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradient Boosting Vs Random Forest**"
      ],
      "metadata": {
        "id": "SdDoKCGF2oui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ‚úÖ Similarities\n",
        "\n",
        "| Aspect                                     | Description                                                                          |\n",
        "| ------------------------------------------ | ------------------------------------------------------------------------------------ |\n",
        "| üå≤ **Based on Decision Trees**             | Both use **decision trees** as their base learners.                                  |\n",
        "| üì¶ **Ensemble Models**                     | Both are **ensemble learning methods** ‚Äî they combine the outputs of multiple trees. |\n",
        "| üè∑Ô∏è **Handle Classification & Regression** | Both can be used for **classification** or **regression** problems.                  |\n",
        "| üìä **Non-linear Relationships**            | Both handle **non-linear data patterns** well.                                       |\n",
        "| üõ†Ô∏è **Reduce Variance/Overfitting**        | Both help avoid overfitting compared to single decision trees.                       |\n",
        "| üß† **Provide Feature Importance**          | Both can rank **features by importance** in prediction.                              |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå Key Differences\n",
        "\n",
        "| Feature           | **Random Forest**                                         | **Gradient Boosting**                                            |\n",
        "| ----------------- | --------------------------------------------------------- | ---------------------------------------------------------------- |\n",
        "| üîÑ Tree Training  | **Parallel**: All trees trained **independently**         | **Sequential**: Each tree fixes errors from the **previous one** |\n",
        "| üß† Learning Style | Uses **bagging**: averages predictions of many deep trees | Uses **boosting**: adds corrections step-by-step                 |\n",
        "| üî¢ Output         | **Average of all trees‚Äô outputs**                         | **Weighted sum** of all trees                                    |\n",
        "| üê¢ Speed          | Faster to train (can be parallelized)                     | Slower to train (sequential learning)                            |\n",
        "| üéØ Focus          | **Reduces variance** (by averaging)                       | **Reduces bias** (by focusing on mistakes)                       |\n",
        "| ‚öôÔ∏è Tuning Effort  | Less sensitive to hyperparameters                         | More sensitive (especially `learning_rate` and `n_estimators`)   |\n",
        "| üß™ Robustness     | More robust to outliers and noise                         | Can **overfit** if not tuned properly                            |\n",
        "\n",
        "---\n",
        "\n",
        "## üîç In Short:\n",
        "\n",
        "| Model                 | Think of it as...                                          |\n",
        "| --------------------- | ---------------------------------------------------------- |\n",
        "| **Random Forest**     | A **voting group** of trees ‚Äî each tree gets an equal say. |\n",
        "| **Gradient Boosting** | A **team of experts**, each fixing the last one‚Äôs mistake. |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like a **visual comparison or a code-based comparison** next?\n"
      ],
      "metadata": {
        "id": "rjmca79d2lJ4"
      }
    }
  ]
}