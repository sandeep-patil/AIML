{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸŒ² Random Forest Regression\n",
        "\n",
        "### âœ… What Is It?\n",
        "\n",
        "> A **Random Forest** is a group (forest) of **Decision Trees**, each trained on a slightly different version of your dataset.\n",
        "> It averages their predictions to produce a more stable and accurate result.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¯ Why It Works So Well\n",
        "\n",
        "| Feature               | Benefit                                  |\n",
        "| --------------------- | ---------------------------------------- |\n",
        "| **Multiple trees**    | Reduces overfitting from any single tree |\n",
        "| **Randomness**        | Improves generalization                  |\n",
        "| **Ensemble learning** | Combines weak learners into a strong one |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¦ Step 2: How It Works Internally\n",
        "\n",
        "1. **Creates many trees** (e.g., 100)\n",
        "2. For each tree:\n",
        "\n",
        "   * Trains on a **random subset of rows** (bootstrap sample)\n",
        "   * Considers only a **random subset of features** at each split\n",
        "3. Each tree gives a prediction\n",
        "4. Final prediction = **average of all tree predictions**\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§® Step 3: Mathematics Behind It\n",
        "\n",
        "### Each tree:\n",
        "\n",
        "Minimizes **Mean Squared Error (MSE)** as before:\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "### The forest:\n",
        "\n",
        "Combines predictions:\n",
        "\n",
        "$$\n",
        "\\hat{y}_{\\text{final}} = \\frac{1}{T} \\sum_{t=1}^{T} \\hat{y}_t\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $T$ = number of trees\n",
        "* $\\hat{y}_t$ = prediction from tree $t$\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¤– Compared to Decision Tree\n",
        "\n",
        "| Feature          | Decision Tree | Random Forest          |\n",
        "| ---------------- | ------------- | ---------------------- |\n",
        "| Model type       | Single model  | Ensemble of many trees |\n",
        "| Overfitting risk | âœ… High        | âŒ Low                  |\n",
        "| Accuracy         | âš ï¸ Unstable   | âœ… High                 |\n",
        "| Explainability   | âœ… Easy        | âš ï¸ Harder              |\n",
        "| Performance      | Fast to train | Slower but robust      |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ”§ Why It's Great for You (#info + #JD)\n",
        "\n",
        "* **Structured data + mixed feature types** â†’ perfect match\n",
        "* Robust for:\n",
        "\n",
        "  * **Price prediction**\n",
        "  * **Warranty estimation**\n",
        "  * **Customer behavior**\n",
        "* Works well with **limited data** and **no feature scaling**\n",
        "* Can extract **feature importances** (which features mattered most)\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hxcg0ODs0QTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ğŸŒ² Advanced Info on Random Forest Regression\n",
        "\n",
        "### ğŸ”§ 1. **Built-In Pruning (Bagging + Randomization)**\n",
        "\n",
        "Random Forest doesnâ€™t need traditional **post-pruning** like a single decision tree, because it combats overfitting by:\n",
        "\n",
        "| Mechanism                 | Effect                                   |\n",
        "| ------------------------- | ---------------------------------------- |\n",
        "| **Bootstrap Sampling**    | Trees trained on random data subsets     |\n",
        "| **Random Feature Subset** | Forces trees to learn different patterns |\n",
        "| **Tree Depth Limiting**   | Optional â€” you can still use `max_depth` |\n",
        "| **Averaging Output**      | Smooths predictions, reduces variance    |\n",
        "\n",
        "âœ… So pruning = **not needed manually**, but you **can still apply limits** for faster training or less complexity.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” 2. **Key Hyperparameters**\n",
        "\n",
        "| Parameter           | Description                                     |\n",
        "| ------------------- | ----------------------------------------------- |\n",
        "| `n_estimators`      | Number of trees (e.g., 100, 500)                |\n",
        "| `max_depth`         | Limit depth of each tree                        |\n",
        "| `min_samples_split` | Minimum samples needed to split a node          |\n",
        "| `min_samples_leaf`  | Minimum samples in a leaf node                  |\n",
        "| `max_features`      | How many features to consider at each split     |\n",
        "| `bootstrap`         | Whether to bootstrap the samples (True/False)   |\n",
        "| `oob_score`         | Use out-of-bag samples to estimate test score âœ… |\n",
        "\n",
        "âœ… For your case (car resale):\n",
        "\n",
        "* `n_estimators=100` is a good starting point\n",
        "* Use `max_depth` or `min_samples_leaf` to reduce overfitting\n",
        "* Enable `oob_score=True` to get a built-in CV estimate\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ“Š 3. **Feature Importance**\n",
        "\n",
        "Random Forest can tell you **which features were most useful** across all trees:\n",
        "\n",
        "```python\n",
        "model.feature_importances_\n",
        "```\n",
        "\n",
        "Use it to:\n",
        "\n",
        "* **Drop low-value features** to simplify model\n",
        "* **Explain model behavior** to business/stakeholders\n",
        "* Feed into further **XAI tools like SHAP**\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§  4. **How It Helps in Automotive Applications (#JD)**\n",
        "\n",
        "| Use Case                        | Why Random Forest Helps                |\n",
        "| ------------------------------- | -------------------------------------- |\n",
        "| Vehicle resale price estimation | Non-linear, mixed features             |\n",
        "| Predictive maintenance          | Robust to noise in sensor logs         |\n",
        "| Warranty cost forecasting       | Captures interactions + outliers       |\n",
        "| Customer segmentation           | Tree logic fits discrete decisions     |\n",
        "| Telematics data usage           | Handles high dimensional, raw features |\n",
        "\n",
        "---\n",
        "\n",
        "### âš–ï¸ 5. **Comparison with Decision Tree**\n",
        "\n",
        "| Aspect           | Decision Tree         | Random Forest       |\n",
        "| ---------------- | --------------------- | ------------------- |\n",
        "| Overfitting Risk | âœ… High                | âŒ Lower (averaging) |\n",
        "| Interpretability | âœ… Visual tree         | âš ï¸ Less transparent |\n",
        "| Accuracy         | âš ï¸ Depends on pruning | âœ… Usually better    |\n",
        "| Feature Ranking  | âŒ None                | âœ… Built-in          |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” 6. **When Not to Use It**\n",
        "\n",
        "* You need **fully interpretable rules**\n",
        "* Youâ€™re dealing with **massive datasets and need very fast inference**\n",
        "* You prefer a **compact model** (e.g., mobile/embedded use)\n",
        "\n",
        "> Tip: Use **Gradient Boosting (like XGBoost)** or **Tree Distillation** if needed later\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0Jj0kjRW1m5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸ”§ Hyperparameter Comparison: Decision Tree vs Random Forest\n",
        "\n",
        "| Hyperparameter      | Decision Tree                    | Random Forest                             | Effect / Purpose                               |\n",
        "| ------------------- | -------------------------------- | ----------------------------------------- | ---------------------------------------------- |\n",
        "| `criterion`         | `'squared_error'` (default)      | `'squared_error'` (default)               | Loss function to minimize (MSE for regression) |\n",
        "| `max_depth`         | âœ… You control max tree depth     | âœ… Controls depth of each tree             | Prevents overfitting; sets complexity          |\n",
        "| `min_samples_split` | âœ… Min samples to split a node    | âœ… Per tree                                | Higher = less complex, prevents small branches |\n",
        "| `min_samples_leaf`  | âœ… Min samples in a leaf          | âœ… Per tree                                | Enforces minimum leaf size, avoids overfitting |\n",
        "| `max_features`      | âŒ Uses all features              | âœ… Defaults to `sqrt(n_features)`          | Adds randomness; improves generalization       |\n",
        "| `max_leaf_nodes`    | âœ… Max number of leaf nodes       | âœ… Per tree                                | Upper bound on tree size                       |\n",
        "| `random_state`      | âœ… Used for reproducibility       | âœ… Also used for bootstrapping             | Fix randomness for reproducible results        |\n",
        "| `splitter`          | `'best'` or `'random'`           | âŒ Not applicable                          | Controls how best split is chosen              |\n",
        "| `ccp_alpha`         | âœ… Cost complexity pruning (post) | âŒ Rarely used in RF (handled by ensemble) | Prune weak branches post training              |\n",
        "| `n_estimators`      | âŒ Only one tree                  | âœ… Number of trees in the forest           | More trees = better stability (but slower)     |\n",
        "| `bootstrap`         | âŒ Not used                       | âœ… Default = True                          | Use sampling with replacement per tree         |\n",
        "| `oob_score`         | âŒ Not available                  | âœ… Optional (True/False)                   | Estimate test score using unused rows          |\n",
        "| `n_jobs`            | âŒ Not needed                     | âœ… Enables parallel training               | `-1` = use all cores                           |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” Key Takeaways:\n",
        "\n",
        "* **Decision Tree** is simple and fast, but prone to overfitting without tuning\n",
        "* **Random Forest** adds **randomness + averaging**, making it **much more robust**\n",
        "* Use **`max_depth`, `min_samples_leaf`** in both to control complexity\n",
        "* Use **`n_estimators`, `max_features`, `bootstrap`** in RF for boosting performance and generalization\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3jEXsYJT2PQ1"
      }
    }
  ]
}